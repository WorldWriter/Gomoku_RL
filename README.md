# AlphaZero Gomoku

A reinforcement learning project implementing AlphaZero for Gomoku (Five in a Row). This project supports multiple board sizes (5Ã—5, 10Ã—10, 15Ã—15) for progressive training and learning.

## ğŸ“š Documentation

**æ–°æ‰‹ï¼Ÿ** æˆ‘ä»¬ä¸ºé›¶åŸºç¡€å­¦ä¹ è€…å‡†å¤‡äº†å®Œæ•´çš„ä¸­æ–‡æŠ€æœ¯æ–‡æ¡£ï¼

ğŸ‘‰ **[æŸ¥çœ‹å®Œæ•´æ–°æ‰‹æ•™ç¨‹](./docs/)** - ä»é›¶å¼€å§‹å­¦ä¹ AlphaZero

æ–‡æ¡£åŒ…å«ï¼š
- ğŸ“ åŸºç¡€æ¦‚å¿µï¼ˆåƒæ•™å°æœ‹å‹ä¸€æ ·è®²è§£ï¼‰
- ğŸ“ é¡¹ç›®ç»“æ„è¯¦è§£
- ğŸ§  æ ¸å¿ƒç®—æ³•åŸç†ï¼ˆMCTSã€ResNetã€è‡ªæˆ‘å¯¹å¼ˆï¼‰
- ğŸš€ è®­ç»ƒå…¨æµç¨‹ï¼ˆæ‰‹æŠŠæ‰‹æ•™å­¦ï¼‰
- ğŸ’» ä»£ç èµ°è¯»ï¼ˆé€è¡Œè®²è§£ï¼‰
- ğŸ¯ å®æˆ˜ç»ƒä¹ ï¼ˆåŠ¨æ‰‹è®­ç»ƒï¼‰

## Features

- **AlphaZero Algorithm**: Complete implementation with MCTS and deep neural networks
- **Multi-size Support**: Train on 5Ã—5, 10Ã—10, or 15Ã—15 boards
- **Cross-platform**: Works on both Mac (CPU) and Windows (CUDA GPU)
- **Self-play Training**: Generates training data through self-play
- **Human vs AI**: Play against trained models
- **Model Evaluation**: Evaluate models against random baseline or compare models

## Project Structure

```
Gomoku_RL/
â”œâ”€â”€ gomoku/              # Game logic
â”‚   â”œâ”€â”€ board.py         # Board implementation
â”‚   â””â”€â”€ game.py          # Game wrapper
â”œâ”€â”€ alphazero/           # AlphaZero implementation
â”‚   â”œâ”€â”€ network.py       # ResNet policy-value network
â”‚   â”œâ”€â”€ mcts.py          # Monte Carlo Tree Search
â”‚   â”œâ”€â”€ self_play.py     # Self-play data generation
â”‚   â””â”€â”€ trainer.py       # Training loop
â”œâ”€â”€ configs/             # Configuration files
â”‚   â”œâ”€â”€ config_5x5.py    # 5Ã—5 board config
â”‚   â”œâ”€â”€ config_10x10.py  # 10Ã—10 board config
â”‚   â””â”€â”€ config_15x15.py  # 15Ã—15 board config
â”œâ”€â”€ utils/               # Utility functions
â”‚   â”œâ”€â”€ device.py        # CUDA/CPU detection
â”‚   â””â”€â”€ logger.py        # Logging utilities
â”œâ”€â”€ train.py             # Main training script
â”œâ”€â”€ play.py              # Human vs AI interface
â”œâ”€â”€ evaluate.py          # Model evaluation
â””â”€â”€ check_env.py         # Environment check
```

## Installation

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- CUDA (optional, for GPU training on Windows)

### Setup

1. Clone the repository:
```bash
git clone https://github.com/yourusername/Gomoku_RL.git
cd Gomoku_RL
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Check your environment:
```bash
python check_env.py
```

## Usage

### 1. Training

Train a model on a specific board size:

```bash
# Quick training on 5Ã—5 (for testing)
python train.py --board_size 5

# Medium training on 10Ã—10
python train.py --board_size 10

# Full training on 15Ã—15 (requires GPU)
python train.py --board_size 15
```

Additional options:
```bash
# Specify number of iterations
python train.py --board_size 5 --iterations 50

# Resume from checkpoint
python train.py --board_size 10 --resume models/10x10/checkpoint_iter_50.pth

# Force CPU training
python train.py --board_size 5 --cpu
```

### 2. Play Against AI

Play against a trained model:

```bash
python play.py --checkpoint models/5x5/checkpoint_latest.pth --board_size 5

# Play as black (first)
python play.py --checkpoint models/10x10/checkpoint_latest.pth --board_size 10 --human_first

# Adjust AI strength (fewer simulations = weaker)
python play.py --checkpoint models/15x15/checkpoint_latest.pth --board_size 15 --simulations 200
```

### 3. Evaluate Models

Evaluate model against random player:

```bash
python evaluate.py --checkpoint models/5x5/checkpoint_latest.pth --board_size 5 --num_games 100
```

Compare two models:

```bash
python evaluate.py --checkpoint models/10x10/checkpoint_iter_100.pth \
                   --checkpoint2 models/10x10/checkpoint_iter_50.pth \
                   --baseline model --board_size 10 --num_games 50
```

## Training Strategy

### Phase 1: 5Ã—5 Board (Quick Validation)
- **Purpose**: Verify algorithm correctness
- **Time**: A few hours on CPU
- **Configuration**: 4 ResNet blocks, 200 MCTS simulations
- **Expected**: Agent learns basic patterns

### Phase 2: 10Ã—10 Board (Medium Scale)
- **Purpose**: Test scalability
- **Time**: 1-2 days on GPU
- **Configuration**: 8 ResNet blocks, 400 MCTS simulations
- **Expected**: More sophisticated strategies

### Phase 3: 15Ã—15 Board (Full Gomoku)
- **Purpose**: Achieve strong AI performance
- **Time**: Several days on GPU
- **Configuration**: 10 ResNet blocks, 800 MCTS simulations
- **Expected**: Near-expert level play

## Configuration

Each board size has its own configuration file in `configs/`:

- `config_5x5.py`: Fast training, suitable for CPU
- `config_10x10.py`: Medium training, GPU recommended
- `config_15x15.py`: Full training, GPU required

Key parameters you can adjust:
- `NUM_SIMULATIONS`: MCTS simulations per move
- `NUM_SELF_PLAY_GAMES`: Games per training iteration
- `BATCH_SIZE`: Training batch size
- `LEARNING_RATE`: Neural network learning rate
- `NUM_RES_BLOCKS`: Number of residual blocks in network

## Cross-Platform Training

### Development on Mac (CPU)
```bash
# Quick iterations on 5Ã—5
python train.py --board_size 5 --iterations 20
```

### Training on Windows (GPU)
```bash
# Full training on larger boards
python train.py --board_size 15 --iterations 500
```

The code automatically detects CUDA availability and uses GPU when available.

## Algorithm Details

### AlphaZero Components

1. **Neural Network**: ResNet architecture with two heads
   - Policy head: Outputs move probabilities
   - Value head: Outputs position evaluation

2. **MCTS**: Monte Carlo Tree Search with UCB selection
   - Upper Confidence Bound formula: Q + c_puct Ã— P Ã— âˆšN_parent / (1 + N)
   - Dirichlet noise at root for exploration

3. **Self-play**: Generate training data
   - Play games using current network + MCTS
   - Store (state, policy, outcome) tuples
   - Data augmentation with symmetries (rotations, flips)

4. **Training**: Update neural network
   - Policy loss: Cross-entropy between MCTS policy and network policy
   - Value loss: MSE between game outcome and network value
   - Combined loss optimized with Adam

## Tips for Training

1. **Start Small**: Always test on 5Ã—5 first to verify everything works
2. **Monitor Loss**: Training loss should decrease steadily
3. **Save Checkpoints**: Models are saved every N iterations
4. **GPU Memory**: Adjust batch size if running out of memory
5. **Training Time**: Be patient - good models take time to train

## Troubleshooting

### CUDA Out of Memory
- Reduce `BATCH_SIZE` in config
- Reduce `NUM_SIMULATIONS` for MCTS
- Use smaller network (fewer `NUM_RES_BLOCKS`)

### Training Too Slow
- Ensure CUDA is being used (check with `python check_env.py`)
- Reduce `NUM_SELF_PLAY_GAMES`
- Use smaller board size for testing

### Model Not Learning
- Increase `NUM_SIMULATIONS` for better MCTS policies
- Check loss curves - they should decrease
- Train for more iterations
- Try different learning rates

## Future Improvements

- [ ] Parallel self-play with multiprocessing
- [ ] Learning rate scheduling
- [ ] Arena evaluation (new model vs old model)
- [ ] Web interface for playing
- [ ] Pre-trained models for download
- [ ] TensorBoard integration for monitoring

## References

- [AlphaGo Zero Paper](https://www.nature.com/articles/nature24270)
- [AlphaZero Paper](https://arxiv.org/abs/1712.01815)
- [MCTS Survey](https://ieeexplore.ieee.org/document/6145622)

## License

MIT License - feel free to use this project for learning and experimentation.

## Acknowledgments

This project is a learning implementation of the AlphaZero algorithm for educational purposes.
