# 06 - 实战练习

> 🎯 **学习目标**：通过实战练习巩固所学知识
> ⏱️ **预计时间**：2-3小时（包含训练时间）
> 📌 **适合人群**：完成了前面文档的学习者

---

## 练习目标

通过4个递进式练习，你将：
- ✅ 完整走通AlphaZero训练流程
- ✅ 学会调整超参数
- ✅ 掌握模型评估方法
- ✅ 理解训练对AI实力的影响

---

## 练习1：快速验证（30分钟）

### 🎯 目标
验证环境和代码是否正常工作

### 📋 步骤

#### Step 1: 环境检测

```bash
cd Gomoku_RL
python check_env.py
```

**期望输出**：
```
All tests passed! ✓
You can now run training with: python train.py --board_size 5
```

**如果失败**：回到 [04_训练全流程.md](./04_训练全流程.md) 的环境准备部分

#### Step 2: 快速训练（5轮）

```bash
python train.py --board_size 5 --iterations 5
```

**观察输出**：
```
Iteration 1/5
[1/3] Generating self-play games...
  Generated 50/50 games
After augmentation: 12000 examples

[2/3] Training neural network...
  Epoch 1/5: Loss = 2.1456
  Epoch 2/5: Loss = 1.9823
  ...

Iteration 1 completed. Avg Loss: 1.8974
```

**检查点**：
- [ ] Loss在下降（2.1 → 1.9 → 1.8）
- [ ] 模型保存成功
- [ ] 没有报错

#### Step 3: 验证模型

```bash
ls models/5x5/
```

**应该看到**：
```
checkpoint_iter_1.pth
checkpoint_iter_2.pth
checkpoint_iter_3.pth
checkpoint_iter_4.pth
checkpoint_iter_5.pth
checkpoint_latest.pth
```

#### Step 4: 对弈测试

```bash
python play.py \
    --checkpoint models/5x5/checkpoint_latest.pth \
    --board_size 5 \
    --human_first \
    --simulations 100
```

**试着下一盘**：
```
Move 1
   0  1  2  3  4
 0  .  .  .  .  .
 1  .  .  .  .  .
 2  .  .  .  .  .
 3  .  .  .  .  .
 4  .  .  .  .  .

Your move (row col): 2 2
```

**预期**：
- AI能下棋（虽然可能很弱）
- 不会下非法位置
- 游戏能正常结束

### ✅ 成功标准

- [ ] 训练完成5轮
- [ ] Loss有下降趋势
- [ ] 能和AI下完一局棋

### 💡 疑难解答

**Q: Loss不下降怎么办？**
A: 5轮太少，这是正常的。继续练习2。

**Q: AI下得很烂？**
A: 只训练了5轮，当然弱！等练习2训练更多轮。

---

## 练习2：参数实验（45分钟）

### 🎯 目标
理解超参数对训练的影响

### 📋 任务

#### 实验A：基准训练

```bash
python train.py --board_size 5 --iterations 20
```

**记录**：
- 最终Loss: _______
- 训练时间: _______

#### 实验B：减少MCTS模拟次数

编辑 `configs/config_5x5.py`：

```python
# 修改前
NUM_SIMULATIONS = 200

# 修改后
NUM_SIMULATIONS = 100  # 减半
```

```bash
# 重新训练
python train.py --board_size 5 --iterations 20
```

**记录**：
- 最终Loss: _______
- 训练时间: _______（应该更快）

**对比**：
- 速度：实验B应该快约 ____%
- Loss：实验B的Loss可能 [更高/更低/相似]

#### 实验C：增加网络深度

编辑 `configs/config_5x5.py`：

```python
# 恢复MCTS模拟次数
NUM_SIMULATIONS = 200

# 修改网络
NUM_RES_BLOCKS = 6  # 原来4
```

```bash
python train.py --board_size 5 --iterations 20
```

**记录**：
- 最终Loss: _______
- 训练时间: _______（应该更慢）
- 模型文件大小: _______ (ls -lh models/5x5/checkpoint_latest.pth)

#### 实验D：调整学习率

```python
# 恢复网络深度
NUM_RES_BLOCKS = 4

# 修改学习率
LEARNING_RATE = 0.01  # 原来0.001，大10倍
```

```bash
python train.py --board_size 5 --iterations 20
```

**记录**：
- 最终Loss: _______
- Loss是否稳定下降？: [是/否]

### 📊 实验记录表

| 实验 | MCTS | ResBlock | LR | Loss | 时间 |
|------|------|----------|----|----- |------|
| A | 200 | 4 | 0.001 |  |  |
| B | 100 | 4 | 0.001 |  |  |
| C | 200 | 6 | 0.001 |  |  |
| D | 200 | 4 | 0.01 |  |  |

### 🤔 思考题

1. **MCTS模拟次数的作用**：
   - 为什么减少模拟次数会更快？
   - 对Loss有什么影响？

   <details>
   <summary>答案提示</summary>
   模拟次数少→自我对弈快→但策略质量低→Loss可能更高
   </details>

2. **网络深度的权衡**：
   - 更深的网络有什么好处？
   - 有什么代价？

   <details>
   <summary>答案提示</summary>
   更深→表达能力强→Loss可能更低<br>
   但：训练慢、内存大、可能过拟合
   </details>

3. **学习率的影响**：
   - 学习率太大会怎样？
   - 太小呢？

   <details>
   <summary>答案提示</summary>
   太大→Loss震荡、不稳定<br>
   太小→收敛慢、需要更多轮
   </details>

### ✅ 成功标准

- [ ] 完成4个实验
- [ ] 理解每个参数的作用
- [ ] 找到最佳配置（Loss最低）

---

## 练习3：完整训练和评估（60分钟）

### 🎯 目标
训练一个像样的AI并评估其实力

### 📋 步骤

#### Step 1: 完整训练

使用最优配置（从练习2得出）训练100轮：

```bash
python train.py --board_size 5 --iterations 100
```

**预计时间**：Mac CPU约5-6小时

**可以挂后台**：
```bash
# Mac/Linux
nohup python train.py --board_size 5 --iterations 100 > train.log 2>&1 &

# 查看进度
tail -f train.log
```

#### Step 2: 监控训练

**每10轮检查一次**：

```bash
# 查看Loss趋势
grep "Avg Loss" logs/*.log | tail -20

# 查看进度
ls models/5x5/checkpoint_iter_*.pth | wc -l
```

**画Loss曲线**（可选）：

```python
# plot_loss.py
import re
import matplotlib.pyplot as plt

# 读取日志
with open('logs/AlphaZero_5x5_*.log', 'r') as f:
    lines = f.readlines()

losses = []
for line in lines:
    match = re.search(r'Avg Loss: ([\d.]+)', line)
    if match:
        losses.append(float(match.group(1)))

# 绘图
plt.plot(losses)
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.title('Training Loss Curve')
plt.grid(True)
plt.savefig('loss_curve.png')
print("Saved to loss_curve.png")
```

#### Step 3: 评估vs随机

```bash
python evaluate.py \
    --checkpoint models/5x5/checkpoint_latest.pth \
    --board_size 5 \
    --baseline random \
    --num_games 100
```

**期望结果**：

```
Evaluation Results
==================
Total games: 100
AI wins: 95 (95.0%)
Random wins: 3 (3.0%)
Draws: 2 (2.0%)
==================
```

**记录**：
- AI胜率: _____%

#### Step 4: 对比不同迭代

```bash
# 早期 vs 晚期
python evaluate.py \
    --checkpoint models/5x5/checkpoint_iter_20.pth \
    --checkpoint2 models/5x5/checkpoint_iter_100.pth \
    --baseline model \
    --board_size 5 \
    --num_games 50
```

**期望**：
- 迭代100的模型胜率 > 60%

#### Step 5: 人机对弈

```bash
python play.py \
    --checkpoint models/5x5/checkpoint_latest.pth \
    --board_size 5 \
    --human_first \
    --simulations 400  # 增加思考深度
```

**下3局棋，记录**：
- 第1局：你 [赢/输]
- 第2局：你 [赢/输]
- 第3局：你 [赢/输]

### 📊 评估报告

填写下表：

| 指标 | 值 |
|------|---|
| 训练轮数 | 100 |
| 最终Loss |  |
| vs随机胜率 |  |
| iter_20 vs iter_100 |  |
| vs你的胜率 |  |

### ✅ 成功标准

- [ ] 完成100轮训练
- [ ] vs随机胜率 > 90%
- [ ] 能感觉到AI进步
- [ ] Loss < 1.2

---

## 练习4：挑战大棋盘（30分钟设置）

### 🎯 目标
在GPU上训练10×10棋盘（如果有GPU）

### 📋 步骤

#### Step 1: 检查GPU

```bash
python check_env.py
```

**如果有GPU**：
```
CUDA Available: True
GPU: NVIDIA RTX ...
```

**如果没有GPU**：
- Mac用户：跳过此练习，或用CPU训练10×10（很慢）
- Windows用户：考虑使用Google Colab或租用GPU

#### Step 2: 启动10×10训练

```bash
python train.py --board_size 10 --iterations 200
```

**配置**：
- MCTS模拟：400次
- ResNet块：8层
- Batch：64

**预计时间**：
- GPU：约1-2天
- CPU：不推荐（数周）

#### Step 3: 监控训练

```bash
# 实时查看
tail -f logs/AlphaZero_10x10_*.log

# 检查进度
watch -n 60 'ls models/10x10/*.pth | wc -l'
```

#### Step 4: 中期评估

**每50轮评估一次**：

```bash
# 迭代50
python evaluate.py \
    --checkpoint models/10x10/checkpoint_iter_50.pth \
    --board_size 10 \
    --baseline random \
    --num_games 50

# 迭代100
python evaluate.py \
    --checkpoint models/10x10/checkpoint_iter_100.pth \
    --board_size 10 \
    --baseline random \
    --num_games 50
```

**记录**：
| 迭代 | vs随机胜率 |
|------|-----------|
| 50 |  |
| 100 |  |
| 150 |  |
| 200 |  |

### ✅ 成功标准（可选）

- [ ] 成功启动10×10训练
- [ ] 训练至少50轮
- [ ] Loss稳定下降
- [ ] vs随机胜率 > 95%

---

## 🏆 综合挑战（高级）

完成以上所有练习后，挑战以下任务：

### 挑战1：调出最强5×5 AI

**目标**：vs随机胜率99%+

**建议**：
- 训练200轮
- MCTS模拟400次
- ResNet 6-8层
- 数据增强

### 挑战2：实现对战平台

创建一个脚本让两个模型对战：

```python
# arena.py
def battle(model1_path, model2_path, num_games=20):
    """两个模型对战"""
    # TODO: 实现
    pass
```

### 挑战3：可视化MCTS树

绘制MCTS搜索树的访问情况：

```python
def visualize_mcts_tree(mcts, save_path):
    """保存MCTS树的可视化"""
    # TODO: 使用graphviz或matplotlib
    pass
```

### 挑战4：改进算法

选择一个方向：

1. **虚拟损失**：并行MCTS
2. **Dirichlet噪声**：根节点探索
3. **温度衰减**：更精细的探索策略
4. **价值网络归一化**：改进训练稳定性

---

## 📝 学习总结

完成所有练习后，回答以下问题：

### 理论理解

1. **AlphaZero的三大组件是什么？**
   <details>
   <summary>答案</summary>
   神经网络、MCTS、自我对弈
   </details>

2. **MCTS的四个步骤是？**
   <details>
   <summary>答案</summary>
   Selection、Expansion、Evaluation、Backpropagation
   </details>

3. **为什么要用ResNet而不是普通CNN？**
   <details>
   <summary>答案</summary>
   残差连接允许梯度直接传播，可以训练很深的网络
   </details>

### 实践经验

1. **你观察到的Loss下降规律是？**

2. **哪个参数对训练速度影响最大？**

3. **如何判断训练是否成功？**

### 改进方向

1. **如果让你优化训练，你会怎么做？**

2. **你遇到了什么问题？怎么解决的？**

---

## 🎉 恭喜完成所有练习！

你现在应该：
- ✅ 理解AlphaZero的完整流程
- ✅ 能够独立训练模型
- ✅ 会调整超参数
- ✅ 能够评估和分析模型

### 下一步

- **深入研究**：阅读AlphaZero论文
- **扩展项目**：实现其他棋类游戏
- **优化算法**：尝试最新的改进（MuZero、EfficientZero）
- **分享经验**：写博客、发GitHub

---

**项目完成度检查**：

- [ ] 完成练习1（快速验证）
- [ ] 完成练习2（参数实验）
- [ ] 完成练习3（完整训练）
- [ ] 完成练习4（大棋盘，可选）
- [ ] 至少一个综合挑战

**学习成果**：

我学会了：
- [ ] AlphaZero的原理
- [ ] PyTorch深度学习
- [ ] MCTS搜索算法
- [ ] 强化学习实战
- [ ] 项目工程化

---

**返回主目录** → [00_索引目录.md](./00_索引目录.md)
