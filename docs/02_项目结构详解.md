# 02 - 项目结构详解

> 🎯 **学习目标**：理解每个文件和文件夹的作用
> ⏱️ **预计时间**：20分钟
> 📌 **适合人群**：想了解代码组织的学习者

---

## 第一部分：完整项目目录树

```
Gomoku_RL/                          # 项目根目录
│
├── 📁 gomoku/                      # 五子棋游戏核心逻辑
│   ├── __init__.py                 # 包初始化
│   ├── board.py                    # 棋盘类
│   └── game.py                     # 游戏包装类
│
├── 📁 alphazero/                   # AlphaZero算法实现
│   ├── __init__.py                 # 包初始化
│   ├── network.py                  # 神经网络（ResNet）
│   ├── mcts.py                     # 蒙特卡洛树搜索
│   ├── self_play.py                # 自我对弈
│   └── trainer.py                  # 训练器
│
├── 📁 configs/                     # 配置文件
│   ├── __init__.py                 # 包初始化
│   ├── config_5x5.py               # 5×5棋盘配置
│   ├── config_10x10.py             # 10×10棋盘配置
│   └── config_15x15.py             # 15×15棋盘配置
│
├── 📁 utils/                       # 工具函数
│   ├── __init__.py                 # 包初始化
│   ├── device.py                   # 设备检测（CUDA/CPU）
│   └── logger.py                   # 日志工具
│
├── 📁 docs/                        # 项目文档（你正在看）
│   ├── 00_索引目录.md
│   ├── 01_基础概念_小白入门.md
│   ├── 02_项目结构详解.md         # ← 当前文档
│   ├── 03_核心算法原理.md
│   ├── 04_训练全流程.md
│   ├── 05_代码走读.md
│   └── 06_实战练习.md
│
├── 📁 models/                      # 训练好的模型存储
│   ├── 5x5/                        # 5×5模型
│   ├── 10x10/                      # 10×10模型
│   └── 15x15/                      # 15×15模型
│
├── 📁 logs/                        # 训练日志
│   └── AlphaZero_*.log             # 日志文件
│
├── 🐍 train.py                     # 主训练脚本
├── 🐍 play.py                      # 人机对弈脚本
├── 🐍 evaluate.py                  # 模型评估脚本
├── 🐍 check_env.py                 # 环境检测脚本
│
├── 📄 README.md                    # 项目说明（英文）
├── 📄 README_CN.md                 # 项目说明（中文）
├── 📄 QUICKSTART.md                # 快速开始指南
├── 📄 requirements.txt             # Python依赖
└── 📄 .gitignore                   # Git忽略文件
```

---

## 第二部分：文件夹详解

### 📁 gomoku/ - 游戏核心逻辑

**作用**：定义五子棋的游戏规则和棋盘操作

#### `board.py` - 棋盘类 (289行)
**核心功能**：
- ✅ 管理棋盘状态（哪里有黑子/白子/空位）
- ✅ 判断是否胜利（检测5连珠）
- ✅ 落子和悔棋操作
- ✅ 获取合法走法

**关键类**：
```python
class GomokuBoard:
    def __init__(self, size=15, n_in_row=5):
        # 创建size×size棋盘，n连珠获胜

    def make_move(self, row, col):
        # 在(row, col)位置落子

    def check_win(self, row, col):
        # 检查最后一步是否获胜

    def get_valid_moves(self):
        # 获取所有空位
```

**使用场景**：
- 需要知道当前棋盘状态时
- 判断游戏是否结束时
- 检查某个位置能否下棋时

---

#### `game.py` - 游戏包装类 (205行)
**核心功能**：
- ✅ 把Board包装成AlphaZero需要的格式
- ✅ 提供神经网络输入（特征平面）
- ✅ 数据增强（棋盘旋转、翻转）
- ✅ 统一接口给MCTS使用

**关键类**：
```python
class GomokuGame:
    def get_canonical_board(self):
        # 返回神经网络需要的3通道棋盘：
        # [当前玩家棋子, 对手棋子, 回合标记]

    def get_symmetries(self, board, policy):
        # 生成8种对称棋盘（旋转+翻转）
        # 用于数据增强

    def get_valid_moves(self):
        # 返回合法动作的二进制向量
```

**为什么需要这个类？**
- Board只关心游戏规则
- Game提供AlphaZero需要的标准接口
- 解耦游戏逻辑和AI算法

---

### 📁 alphazero/ - AlphaZero算法核心

**作用**：实现AlphaZero的所有核心组件

#### `network.py` - 神经网络 (191行)
**核心功能**：
- ✅ ResNet残差网络架构
- ✅ 策略头（Policy Head）- 输出每个位置的概率
- ✅ 价值头（Value Head）- 输出局面评分

**关键类**：
```python
class ResidualBlock:
    # 残差块：让网络可以很深但不会梯度消失

class AlphaZeroNet:
    def __init__(self, board_size, num_channels, num_res_blocks):
        # 创建网络
        # - 输入: 3×size×size 棋盘
        # - 输出: 策略 + 价值

    def forward(self, x):
        # 前向传播
        # x -> ResNet -> [policy_logits, value]

    def predict(self, board_state):
        # 单个棋盘预测（不需要梯度）
```

**网络结构**：
```
输入 (3, 15, 15)
  ↓
初始卷积层 (64通道)
  ↓
残差块 × 10层
  ↓
分成两个头
  ↓                    ↓
策略头              价值头
  ↓                    ↓
225个位置概率      [-1, 1]评分
```

**设计要点**：
- **ResNet**：让网络可以堆很深，提取复杂模式
- **双头设计**：同时学习"怎么下"和"谁会赢"
- **批归一化**：加速训练，稳定性更好

---

#### `mcts.py` - 蒙特卡洛树搜索 (251行)
**核心功能**：
- ✅ 实现MCTS的4个步骤（选择、扩展、评估、反向传播）
- ✅ UCB公式平衡探索和利用
- ✅ 根节点温度采样

**关键类**：
```python
class MCTSNode:
    # MCTS树的节点
    def __init__(self, prior_prob):
        self.visit_count = 0           # 访问次数
        self.total_value = 0.0         # 累计价值
        self.prior_prob = prior_prob   # 先验概率（来自网络）
        self.children = {}             # 子节点

    def get_ucb_score(self, parent_N, c_puct):
        # UCB公式：Q + c_puct × P × √N_parent / (1+N)
        # Q: 平均价值（exploitation）
        # 后半部分: 探索奖励（exploration）

class MCTS:
    def get_action_probs(self, game, temperature):
        # 运行num_simulations次模拟
        # 返回每个动作的访问次数分布

    def _simulate(self, game):
        # 一次MCTS模拟的完整流程
```

**MCTS工作流程**：
```
1. Selection（选择）
   从根节点开始，用UCB选最优子节点

2. Expansion（扩展）
   到达叶子节点，扩展所有可能走法

3. Evaluation（评估）
   用神经网络评估叶子节点

4. Backpropagation（反向传播）
   更新路径上所有节点的统计数据
```

**重要参数**：
- `num_simulations`: 模拟次数（越多越强但越慢）
- `c_puct`: 探索常数（通常设为1.0）
- `temperature`: 控制随机性

---

#### `self_play.py` - 自我对弈 (134行)
**核心功能**：
- ✅ 让AI自己和自己下棋
- ✅ 收集训练数据 (state, policy, value)
- ✅ 数据增强

**关键类**：
```python
class SelfPlay:
    def play_game(self, temperature_schedule):
        # 完整下一局棋
        # 返回训练样本：[(state, mcts_policy, outcome), ...]

    def _assign_rewards(self, examples, game_result):
        # 游戏结束后，给每个样本分配真实结果
        # 赢家的样本: value=+1
        # 输家的样本: value=-1

def augment_data(examples, game):
    # 数据增强：8种对称变换
```

**自我对弈流程**：
```
重置棋盘
↓
当前玩家用MCTS选择动作
↓
记录 (棋盘, MCTS策略)
↓
执行动作，切换玩家
↓
继续，直到分出胜负
↓
根据结果填充所有样本的value
↓
返回训练数据
```

**为什么记录MCTS策略而不是最终选择？**
- MCTS访问次数分布比单一动作包含更多信息
- 训练网络模仿MCTS，而不是模仿自己

---

#### `trainer.py` - 训练器 (226行)
**核心功能**：
- ✅ 管理完整训练流程
- ✅ 自我对弈 → 训练网络 → 保存模型
- ✅ 计算Loss并优化

**关键类**：
```python
class Trainer:
    def train(self, num_iterations):
        # 主训练循环
        for iter in range(num_iterations):
            1. 生成自我对弈数据
            2. 训练神经网络
            3. 保存检查点

    def _train_network(self):
        # 训练网络
        policy_loss + value_loss = total_loss

    def _policy_loss(self, pred, target):
        # 策略损失：交叉熵
        # 让网络输出接近MCTS策略

    def _value_loss(self, pred, target):
        # 价值损失：均方误差
        # 让网络预测接近真实结果
```

**训练循环**：
```
迭代1:
  └─ 自我对弈50局 → 收集数据 → 训练网络 → 保存模型
迭代2:
  └─ 用新网络再对弈50局 → 更多数据 → 继续训练 → 保存
...
迭代N:
  └─ 网络越来越强！
```

**损失函数**：
```python
total_loss = policy_loss + value_loss

# 策略损失：让网络学习MCTS的策略
policy_loss = -Σ(mcts_policy × log(network_policy))

# 价值损失：让网络预测准确的胜率
value_loss = (network_value - game_outcome)²
```

---

### 📁 configs/ - 配置文件

**作用**：为不同棋盘尺寸设置超参数

#### `config_5x5.py` - 5×5配置
```python
BOARD_SIZE = 5
N_IN_ROW = 4              # 4连珠（因为棋盘小）

NUM_CHANNELS = 64         # 网络通道数（小）
NUM_RES_BLOCKS = 4        # 残差块数量（少）

NUM_SIMULATIONS = 200     # MCTS模拟次数（少）
NUM_SELF_PLAY_GAMES = 50  # 每轮对弈局数

BATCH_SIZE = 32           # 批大小
NUM_EPOCHS = 5            # 每轮训练轮数
LEARNING_RATE = 0.001     # 学习率
```

**为什么参数小？**
- 5×5棋盘简单，不需要复杂网络
- CPU就能跑，适合快速验证

#### `config_10x10.py` - 10×10配置
```python
BOARD_SIZE = 10
N_IN_ROW = 5              # 标准5连珠

NUM_CHANNELS = 128        # 中等网络
NUM_RES_BLOCKS = 8

NUM_SIMULATIONS = 400     # 中等模拟次数
NUM_SELF_PLAY_GAMES = 100

BATCH_SIZE = 64
```

**适合场景**：
- 验证算法扩展性
- GPU训练（1-2天）

#### `config_15x15.py` - 15×15配置
```python
BOARD_SIZE = 15
N_IN_ROW = 5

NUM_CHANNELS = 256        # 大网络！
NUM_RES_BLOCKS = 10

NUM_SIMULATIONS = 800     # 高模拟次数
NUM_SELF_PLAY_GAMES = 100

BATCH_SIZE = 128          # 大批量（需要GPU）
```

**适合场景**：
- 标准五子棋
- 强GPU训练（数天）

**配置对比表**：

| 参数 | 5×5 | 10×10 | 15×15 |
|------|-----|-------|-------|
| 棋盘大小 | 25格 | 100格 | 225格 |
| 网络参数量 | ~50K | ~500K | ~2M |
| MCTS模拟 | 200次 | 400次 | 800次 |
| 训练时间 | 小时级 | 1-2天 | 数天 |
| 硬件需求 | CPU | GPU推荐 | GPU必需 |

---

### 📁 utils/ - 工具函数

#### `device.py` - 设备检测
**功能**：
- ✅ 自动检测CUDA是否可用
- ✅ 跨平台兼容（Mac CPU / Windows GPU）
- ✅ 打印设备信息

```python
def get_device(prefer_cuda=True):
    # 自动选择CUDA或CPU
    if torch.cuda.is_available() and prefer_cuda:
        return torch.device("cuda")
    else:
        return torch.device("cpu")

def print_device_info():
    # 显示PyTorch版本、CUDA信息、GPU型号等
```

**使用例子**：
```python
from utils import get_device

device = get_device()  # 自动选择
network.to(device)     # 模型搬到对应设备
```

#### `logger.py` - 日志工具
**功能**：
- ✅ 同时输出到文件和控制台
- ✅ 带时间戳的日志
- ✅ 自动创建logs目录

```python
def setup_logger(name, log_dir='logs'):
    # 创建logger
    # 日志保存到 logs/AlphaZero_20250116_123456.log
```

---

## 第三部分：主脚本详解

### 🐍 train.py - 主训练脚本 (127行)

**功能**：统一训练入口

**命令行参数**：
```bash
python train.py \
    --board_size 5 \           # 棋盘大小
    --iterations 100 \         # 训练轮数
    --resume model.pth \       # 继续训练（可选）
    --cpu                      # 强制CPU（可选）
```

**工作流程**：
```
1. 加载配置文件（config_5x5.py）
2. 创建游戏和网络
3. 创建训练器
4. 开始训练循环
5. 保存模型到 models/5x5/
```

**关键代码**：
```python
# 加载配置
config = load_config(board_size)

# 创建游戏
game = GomokuGame(board_size, n_in_row)

# 创建网络
network = AlphaZeroNet(...)

# 训练
trainer = Trainer(network, game, config, device)
trainer.train(num_iterations)
```

---

### 🐍 play.py - 人机对弈脚本 (217行)

**功能**：和训练好的AI下棋

**命令行参数**：
```bash
python play.py \
    --checkpoint models/5x5/checkpoint_latest.pth \
    --board_size 5 \
    --human_first \            # 你先手
    --simulations 400          # AI思考深度
```

**界面示例**：
```
Move 1
   0  1  2  3  4
 0  .  .  .  .  .
 1  .  .  .  .  .
 2  .  . [X] .  .  ← AI下的
 3  .  .  .  .  .
 4  .  .  .  .  .

Your move (row col): 1 1

Move 2
   0  1  2  3  4
 0  .  .  .  .  .
 1  .  O  .  .  .  ← 你下的
 2  .  .  X  .  .
 3  .  .  .  .  .
 4  .  .  .  .  .

AI is thinking...
AI played: (2, 3)
```

**关键类**：
```python
class HumanPlayer:
    def get_action(self, game):
        # 从命令行读取你的输入

class AIPlayer:
    def get_action(self, game):
        # 用MCTS + 网络选择最佳走法
```

---

### 🐍 evaluate.py - 模型评估脚本 (287行)

**功能**：评估模型实力

**两种评估模式**：

#### 模式1：vs 随机玩家
```bash
python evaluate.py \
    --checkpoint model.pth \
    --baseline random \
    --num_games 100
```

**输出**：
```
Evaluation Results
==================
Total games: 100
AI wins: 95 (95.0%)     ← AI应该碾压随机
Random wins: 3 (3.0%)
Draws: 2 (2.0%)
```

#### 模式2：模型对比
```bash
python evaluate.py \
    --checkpoint model_new.pth \
    --checkpoint2 model_old.pth \
    --baseline model \
    --num_games 50
```

**用途**：
- 验证训练是否有进步
- 新模型 vs 旧模型，看胜率提升

---

### 🐍 check_env.py - 环境检测脚本 (95行)

**功能**：运行前检查环境

**检查项目**：
- ✅ Python版本
- ✅ PyTorch版本
- ✅ CUDA是否可用
- ✅ 基本tensor运算
- ✅ 神经网络层测试
- ✅ 反向传播测试

**运行结果示例**：
```bash
$ python check_env.py

=========================
Device Information
=========================
PyTorch Version: 2.7.1
CUDA Available: True
GPU: NVIDIA RTX 3080
Memory: 10.00 GB
=========================

Testing basic operations...
✓ Tensor creation: OK
✓ Matrix multiplication: OK
✓ Neural network layers: OK
✓ Backward pass: OK

All tests passed! ✓
```

**遇到问题怎么办？**
- CUDA不可用：检查CUDA驱动
- 测试失败：检查PyTorch安装

---

## 第四部分：文件调用关系

### 🔄 训练时的调用链

```
train.py (入口)
  ├─ 导入 configs/config_5x5.py (配置)
  ├─ 创建 gomoku.GomokuGame (游戏)
  │    └─ 使用 gomoku.GomokuBoard (棋盘)
  ├─ 创建 alphazero.AlphaZeroNet (网络)
  └─ 创建 alphazero.Trainer (训练器)
       ├─ 调用 alphazero.SelfPlay (自我对弈)
       │    └─ 使用 alphazero.MCTS (搜索)
       │         └─ 调用 network.predict() (推理)
       └─ 调用 _train_network() (训练网络)
```

### 🎮 对弈时的调用链

```
play.py (入口)
  ├─ 加载模型 checkpoint.pth
  ├─ 创建 gomoku.GomokuGame
  ├─ 创建 HumanPlayer (你)
  └─ 创建 AIPlayer (AI)
       └─ 使用 alphazero.MCTS
            └─ 调用 network.predict()
```

### 📊 评估时的调用链

```
evaluate.py (入口)
  ├─ 加载模型
  ├─ 创建 AlphaZeroPlayer (用MCTS)
  ├─ 创建 RandomPlayer (随机基线)
  └─ play_game() 循环多局
```

---

## 第五部分：数据流动

### 📊 训练数据流

```
棋盘状态 (15×15)
  ↓
Game.get_canonical_board()
  ↓
特征平面 (3×15×15)
  [当前玩家, 对手, 回合]
  ↓
Network.forward()
  ↓
策略logits (225,) + 价值标量
  ↓
MCTS使用策略进行搜索
  ↓
访问次数分布 → MCTS策略
  ↓
记录 (state, mcts_policy, None)
  ↓
游戏结束后填充真实结果
  ↓
训练样本 (state, policy, value)
  ↓
DataLoader → 批量
  ↓
计算Loss → 反向传播 → 更新网络
```

### 💾 模型保存格式

```python
checkpoint = {
    'iteration': 100,
    'model_state_dict': network.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss_history': [0.5, 0.4, 0.3, ...],
    'args': config,
}
torch.save(checkpoint, 'checkpoint_iter_100.pth')
```

**包含信息**：
- 网络权重
- 优化器状态（继续训练用）
- 训练历史
- 配置参数

---

## 第六部分：修改代码指南

### 💡 常见修改场景

#### 场景1：调整训练参数
**文件**：`configs/config_5x5.py`
```python
# 想更快训练？减少自我对弈局数
NUM_SELF_PLAY_GAMES = 25  # 原来50

# 想更强AI？增加MCTS模拟次数
NUM_SIMULATIONS = 400  # 原来200
```

#### 场景2：改变网络结构
**文件**：`alphazero/network.py`
```python
class AlphaZeroNet:
    def __init__(self, ...):
        # 改成更大的网络
        self.res_blocks = nn.ModuleList([
            ResidualBlock(num_channels)
            for _ in range(20)  # 原来10
        ])
```

#### 场景3：修改MCTS策略
**文件**：`alphazero/mcts.py`
```python
def get_ucb_score(self, parent_N, c_puct):
    # 调整探索-利用权衡
    c_puct = 2.0  # 原来1.0，更鼓励探索
    ...
```

#### 场景4：添加新的评估指标
**文件**：`evaluate.py`
```python
def evaluate_against_random(...):
    # 添加平均步数统计
    total_moves = 0
    for game in games:
        moves = play_game(...)
        total_moves += len(moves)
    avg_moves = total_moves / num_games
    print(f"Average moves: {avg_moves}")
```

---

## 第七部分：快速定位问题

### 🐛 常见问题和对应文件

| 问题 | 应该查看的文件 |
|------|--------------|
| 训练Loss不下降 | `trainer.py` - 检查学习率、损失函数 |
| AI下棋很弱 | `configs/` - 增加MCTS模拟次数<br>`network.py` - 检查网络是否太小 |
| 训练太慢 | `configs/` - 减少自我对弈局数<br>`self_play.py` - 检查MCTS次数 |
| 内存溢出 | `configs/` - 减小batch_size<br>`trainer.py` - 减少历史数据量 |
| 胜负判断错误 | `gomoku/board.py` - check_win函数 |
| MCTS选择奇怪 | `mcts.py` - UCB公式、温度参数 |
| 模型保存失败 | `trainer.py` - _save_checkpoint |
| CUDA错误 | `utils/device.py` - 设备检测 |

---

## 🎓 知识检测

1. **AlphaZero的核心算法文件是哪些？**
   <details>
   <summary>答案</summary>
   network.py, mcts.py, self_play.py, trainer.py
   </details>

2. **如果想调整5×5的训练速度，应该修改哪个文件？**
   <details>
   <summary>答案</summary>
   configs/config_5x5.py
   </details>

3. **Game和Board的区别是什么？**
   <details>
   <summary>答案</summary>
   Board管理游戏规则，Game提供AlphaZero需要的标准接口
   </details>

4. **训练数据中的policy来自哪里？**
   <details>
   <summary>答案</summary>
   MCTS的访问次数分布，不是神经网络直接输出
   </details>

---

**下一步**：深入理解算法原理 → [03_核心算法原理.md](./03_核心算法原理.md)
