# 05 - ä»£ç èµ°è¯»

> ğŸ¯ **å­¦ä¹ ç›®æ ‡**ï¼šæ·±å…¥ç†è§£å…³é”®ä»£ç çš„å®ç°ç»†èŠ‚
> â±ï¸ **é¢„è®¡æ—¶é—´**ï¼š60åˆ†é’Ÿ
> ğŸ“Œ **é€‚åˆäººç¾¤**ï¼šæƒ³æ·±å…¥ç†è§£ä»£ç çš„å¼€å‘è€…

---

## ç›®å½•
1. [ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­](#ç¬¬ä¸€éƒ¨åˆ†ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­)
2. [MCTSä¸€æ¬¡æ¨¡æ‹Ÿ](#ç¬¬äºŒéƒ¨åˆ†mctsä¸€æ¬¡æ¨¡æ‹Ÿ)
3. [è‡ªæˆ‘å¯¹å¼ˆä¸€å±€æ¸¸æˆ](#ç¬¬ä¸‰éƒ¨åˆ†è‡ªæˆ‘å¯¹å¼ˆä¸€å±€æ¸¸æˆ)
4. [è®­ç»ƒä¸€ä¸ªBatch](#ç¬¬å››éƒ¨åˆ†è®­ç»ƒä¸€ä¸ªbatch)
5. [å…³é”®æ•°æ®ç»“æ„](#ç¬¬äº”éƒ¨åˆ†å…³é”®æ•°æ®ç»“æ„)

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­

### ğŸ“ ä½ç½®ï¼š`alphazero/network.py`

#### 1. è¾“å…¥æ•°æ®å‡†å¤‡

```python
def predict(self, board_state):
    """
    ä¸ºå•ä¸ªæ£‹ç›˜çŠ¶æ€é¢„æµ‹ç­–ç•¥å’Œä»·å€¼

    Args:
        board_state: shape (3, board_size, board_size)
                    é€šé“0: å½“å‰ç©å®¶æ£‹å­
                    é€šé“1: å¯¹æ‰‹æ£‹å­
                    é€šé“2: å›åˆæŒ‡ç¤ºå™¨
    """
    self.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­dropoutç­‰ï¼‰
    with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦ï¼ˆæ¨ç†æ¨¡å¼ï¼‰
        if not isinstance(board_state, torch.Tensor):
            # å¦‚æœæ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºtensor
            board_state = torch.FloatTensor(board_state)

        # æ·»åŠ batchç»´åº¦: (3, 5, 5) -> (1, 3, 5, 5)
        if board_state.dim() == 3:
            board_state = board_state.unsqueeze(0)

        # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰
        board_state = board_state.to(next(self.parameters()).device)

        # å‰å‘ä¼ æ’­
        policy_logits, value = self.forward(board_state)

        # è½¬æ¢policy logitsä¸ºæ¦‚ç‡
        policy = F.softmax(policy_logits, dim=1)
        # softmax: [2.3, 1.5, 0.8, ...] -> [0.35, 0.15, 0.08, ...]
        #                                   (å’Œä¸º1çš„æ¦‚ç‡åˆ†å¸ƒ)

        # è½¬æ¢ä¸ºnumpyå¹¶å»æ‰batchç»´åº¦
        policy = policy.cpu().numpy()[0]  # (25,)
        value = value.cpu().numpy()[0][0]  # æ ‡é‡

    return policy, value
```

**å…³é”®ç‚¹**ï¼š
- `eval()` + `torch.no_grad()`ï¼šæ¨ç†æ¨¡å¼ï¼Œæ›´å¿«ä¸”çœå†…å­˜
- `unsqueeze(0)`ï¼šæ·»åŠ batchç»´åº¦ï¼ˆç½‘ç»œæœŸæœ›batchè¾“å…¥ï¼‰
- `softmax`ï¼šlogitsâ†’æ¦‚ç‡åˆ†å¸ƒ

#### 2. å‰å‘ä¼ æ’­æµç¨‹

```python
def forward(self, x):
    """
    Args:
        x: (batch, 3, board_size, board_size)

    Returns:
        policy_logits: (batch, board_size^2)
        value: (batch, 1)
    """
    # === 1. åˆå§‹å·ç§¯ ===
    out = self.conv_input(x)  # (batch, 3, 5, 5) -> (batch, 64, 5, 5)
    out = self.bn_input(out)  # Batch Normalization
    out = F.relu(out)         # æ¿€æ´»å‡½æ•°

    # === 2. æ®‹å·®å¡” ===
    for res_block in self.res_blocks:
        out = res_block(out)  # (batch, 64, 5, 5) -> (batch, 64, 5, 5)
        #                        é€šé“æ•°å’Œå°ºå¯¸ä¸å˜ï¼Œä½†ç‰¹å¾æ›´æŠ½è±¡

    # === 3. ç­–ç•¥å¤´ ===
    policy = self.policy_conv(out)      # (batch, 64, 5, 5) -> (batch, 2, 5, 5)
    policy = self.policy_bn(policy)
    policy = F.relu(policy)
    policy = policy.view(policy.size(0), -1)  # å±•å¹³: (batch, 2Ã—5Ã—5) = (batch, 50)
    policy = self.policy_fc(policy)     # (batch, 50) -> (batch, 25)

    # === 4. ä»·å€¼å¤´ ===
    value = self.value_conv(out)        # (batch, 64, 5, 5) -> (batch, 1, 5, 5)
    value = self.value_bn(value)
    value = F.relu(value)
    value = value.view(value.size(0), -1)  # å±•å¹³: (batch, 25)
    value = self.value_fc1(value)       # (batch, 25) -> (batch, 256)
    value = F.relu(value)
    value = self.value_fc2(value)       # (batch, 256) -> (batch, 1)
    value = torch.tanh(value)           # é™åˆ¶åœ¨[-1, 1]

    return policy, value
```

**æ•°æ®å½¢çŠ¶å˜åŒ–**ï¼š

```
è¾“å…¥: (1, 3, 5, 5)
  â†“
åˆå§‹å·ç§¯: (1, 64, 5, 5)
  â†“
æ®‹å·®å—Ã—4: (1, 64, 5, 5)  â† å½¢çŠ¶ä¸å˜ï¼Œç‰¹å¾æ›´æ·±
  â†“          â†“
ç­–ç•¥å¤´     ä»·å€¼å¤´
  â†“          â†“
(1, 25)   (1, 1)
```

#### 3. æ®‹å·®å—è¯¦è§£

```python
class ResidualBlock(nn.Module):
    def forward(self, x):
        residual = x  # ä¿å­˜è¾“å…¥ï¼ˆè·³è·ƒè¿æ¥ï¼‰

        # ç¬¬ä¸€ä¸ªå·ç§¯
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)

        # ç¬¬äºŒä¸ªå·ç§¯
        out = self.conv2(out)
        out = self.bn2(out)

        # å…³é”®ï¼šåŠ ä¸Šæ®‹å·®
        out += residual  # â† è¿™å°±æ˜¯"æ®‹å·®è¿æ¥"

        out = F.relu(out)
        return out
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**

```
æ™®é€šç½‘ç»œï¼š
  è¾“å…¥ â†’ å·ç§¯1 â†’ å·ç§¯2 â†’ è¾“å‡º
  å¦‚æœç½‘ç»œå¾ˆæ·±ï¼Œæ¢¯åº¦ä¼šæ¶ˆå¤±

æ®‹å·®ç½‘ç»œï¼š
  è¾“å…¥ â†’ å·ç§¯1 â†’ å·ç§¯2 â†’ (+è¾“å…¥) â†’ è¾“å‡º
           â†“                   â†‘
           â””â”€â”€â”€â”€â”€â”€â”€è·³è·ƒè¿æ¥â”€â”€â”€â”€â”˜

  æ¢¯åº¦å¯ä»¥ç›´æ¥é€šè¿‡è·³è·ƒè¿æ¥ä¼ å›å»ï¼
```

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šMCTSä¸€æ¬¡æ¨¡æ‹Ÿ

### ğŸ“ ä½ç½®ï¼š`alphazero/mcts.py`

#### å®Œæ•´æ¨¡æ‹Ÿæµç¨‹

```python
def _simulate(self, game):
    """
    è¿è¡Œä¸€æ¬¡MCTSæ¨¡æ‹Ÿ

    Args:
        game: å½“å‰æ¸¸æˆçŠ¶æ€ï¼ˆä¼šè¢«ä¿®æ”¹ï¼Œæ‰€ä»¥ä¼ å…¥å‰¯æœ¬ï¼‰

    Returns:
        float: ä»æ ¹èŠ‚ç‚¹è§†è§’çš„ä»·å€¼ä¼°è®¡
    """
    # === Step 0: æ£€æŸ¥æ¸¸æˆæ˜¯å¦å·²ç»“æŸ ===
    game_ended = game.get_game_ended()
    if game_ended != 0:
        # æ¸¸æˆç»“æŸï¼Œç›´æ¥è¿”å›ç»“æœ
        # æ³¨æ„ï¼šè¿”å›è´Ÿå€¼ï¼Œå› ä¸ºæ˜¯ä»å¯¹æ‰‹è§†è§’
        return -game_ended

    # === Step 1: åˆå§‹åŒ–æ ¹èŠ‚ç‚¹ï¼ˆå¦‚æœéœ€è¦ï¼‰ ===
    if self.root is None:
        # ç¬¬ä¸€æ¬¡æ¨¡æ‹Ÿï¼Œåˆ›å»ºæ ¹èŠ‚ç‚¹
        policy, value = self.network.predict(game.get_canonical_board())

        # å±è”½éæ³•åŠ¨ä½œ
        valid_moves = game.get_valid_moves()
        policy = policy * valid_moves  # éæ³•ä½ç½®æ¦‚ç‡å˜0

        # å½’ä¸€åŒ–
        policy_sum = np.sum(policy)
        if policy_sum > 0:
            policy = policy / policy_sum
        else:
            # æ‰€æœ‰åˆæ³•åŠ¨ä½œæ¦‚ç‡ä¸º0ï¼ˆç½•è§ï¼‰ï¼Œç”¨å‡åŒ€åˆ†å¸ƒ
            policy = valid_moves / np.sum(valid_moves)

        # åˆ›å»ºæ ¹èŠ‚ç‚¹å¹¶æ‰©å±•
        action_priors = {a: policy[a] for a in range(len(policy)) if valid_moves[a] > 0}
        self.root = MCTSNode(0)
        self.root.expand(action_priors)

        return -value  # ä»å¯¹æ‰‹è§†è§’è¿”å›

    # === Step 2: Selectionï¼ˆé€‰æ‹©ï¼‰ ===
    node = self.root
    search_path = [node]  # è®°å½•è·¯å¾„ï¼Œç”¨äºåå‘ä¼ æ’­
    action_path = []

    while not node.is_leaf():
        # ç”¨UCBé€‰æ‹©æœ€ä½³å­èŠ‚ç‚¹
        action, node = node.select_child(self.args['c_puct'])
        search_path.append(node)
        action_path.append(action)

        # åœ¨æ¸¸æˆä¸­æ‰§è¡Œè¿™ä¸ªåŠ¨ä½œ
        game.get_next_state(action)

    # === Step 3: Expansion & Evaluationï¼ˆæ‰©å±•å’Œè¯„ä¼°ï¼‰ ===
    game_ended = game.get_game_ended()
    if game_ended != 0:
        # åˆ°è¾¾ç»ˆå±€ï¼Œä½¿ç”¨çœŸå®ç»“æœ
        value = -game_ended
    else:
        # å¶å­èŠ‚ç‚¹ï¼Œç”¨ç½‘ç»œè¯„ä¼°
        policy, value = self.network.predict(game.get_canonical_board())

        # å±è”½éæ³•åŠ¨ä½œ
        valid_moves = game.get_valid_moves()
        policy = policy * valid_moves
        policy_sum = np.sum(policy)
        if policy_sum > 0:
            policy = policy / policy_sum
        else:
            policy = valid_moves / np.sum(valid_moves)

        # æ‰©å±•å­èŠ‚ç‚¹
        action_priors = {a: policy[a] for a in range(len(policy)) if valid_moves[a] > 0}
        node.expand(action_priors)

    # === Step 4: Backpropagationï¼ˆåå‘ä¼ æ’­ï¼‰ ===
    for node in reversed(search_path):
        node.update(value)
        value = -value  # æ¯å±‚åè½¬ï¼ˆç©å®¶åˆ‡æ¢ï¼‰

    return value
```

**å¯è§†åŒ–ä¸€æ¬¡æ¨¡æ‹Ÿ**ï¼š

```
åˆå§‹çŠ¶æ€ï¼ˆç©ºæ£‹ç›˜ï¼‰ï¼š
       æ ¹èŠ‚ç‚¹
       /  |  \
     25ä¸ªå­èŠ‚ç‚¹ï¼ˆéƒ½æœªè®¿é—®ï¼‰

ç¬¬1æ¬¡æ¨¡æ‹Ÿï¼š
  Selection: æ ¹â†’é€‰æ‹©prioræœ€å¤§çš„å­èŠ‚ç‚¹A
  Expansion: æ‰©å±•Açš„å­èŠ‚ç‚¹
  Evaluation: ç½‘ç»œè¯„ä¼°value=+0.1
  Backpropagation:
    A: N=1, V=+0.1
    æ ¹: N=1, V=-0.1

ç¬¬2æ¬¡æ¨¡æ‹Ÿï¼š
  Selection: æ ¹â†’å†æ¬¡é€‰æ‹©ï¼ˆUCBé‡æ–°è®¡ç®—ï¼‰
  å¯èƒ½è¿˜æ˜¯Aï¼ˆprioré«˜ï¼‰ï¼Œä¹Ÿå¯èƒ½æ˜¯Bï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
  ...

ç¬¬200æ¬¡æ¨¡æ‹Ÿåï¼š
       æ ¹èŠ‚ç‚¹(N=200)
       /   |    \
     A(90) B(80) C(30)  â† è®¿é—®æ¬¡æ•°
```

#### UCBé€‰æ‹©è¯¦è§£

```python
def get_ucb_score(self, parent_visit_count, c_puct):
    """
    UCB = Q + U
    Q = exploitationï¼ˆåˆ©ç”¨ï¼‰= å¹³å‡ä»·å€¼
    U = explorationï¼ˆæ¢ç´¢ï¼‰= é¼“åŠ±è®¿é—®å°‘çš„èŠ‚ç‚¹
    """
    # Q: å¹³å‡ä»·å€¼
    q_value = self.value()  # = total_value / visit_count

    # U: æ¢ç´¢å¥–åŠ±
    u_value = c_puct * self.prior_prob * math.sqrt(parent_visit_count) / (1 + self.visit_count)
    #         â†‘      â†‘                  â†‘                                  â†‘
    #       å¸¸æ•°   ç½‘ç»œå»ºè®®           çˆ¶èŠ‚ç‚¹è¶Šå¤šè®¿é—®                   è‡ªå·±è®¿é—®å°‘â†’Uå¤§
    #       (1.0)  (policy)          â†’é¼“åŠ±æ¢ç´¢                        (é¼“åŠ±æ¢ç´¢)

    return q_value + u_value
```

**ä¾‹å­**ï¼š

```python
# çˆ¶èŠ‚ç‚¹è®¿é—®äº†100æ¬¡
parent_N = 100
c_puct = 1.0

# èŠ‚ç‚¹A: è®¿é—®å¤šï¼ŒQé«˜
A.visit_count = 50
A.prior_prob = 0.2
A.total_value = 30
Q_A = 30/50 = 0.6
U_A = 1.0 Ã— 0.2 Ã— âˆš100 / (1+50) = 0.039
UCB_A = 0.6 + 0.039 = 0.639

# èŠ‚ç‚¹B: è®¿é—®å°‘ï¼ŒQä½
B.visit_count = 5
B.prior_prob = 0.1
B.total_value = 2
Q_B = 2/5 = 0.4
U_B = 1.0 Ã— 0.1 Ã— âˆš100 / (1+5) = 0.167
UCB_B = 0.4 + 0.167 = 0.567

# èŠ‚ç‚¹C: æœªè®¿é—®ï¼Œprioré«˜
C.visit_count = 0
C.prior_prob = 0.5
Q_C = 0
U_C = 1.0 Ã— 0.5 Ã— âˆš100 / (1+0) = 5.0
UCB_C = 0 + 5.0 = 5.0  â† æœ€é«˜ï¼ä¼šè¢«é€‰æ‹©

# ç»“è®ºï¼šæœªè®¿é—®ä¸”ç½‘ç»œè®¤ä¸ºå¥½çš„èŠ‚ç‚¹ä¼šè¢«ä¼˜å…ˆæ¢ç´¢
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šè‡ªæˆ‘å¯¹å¼ˆä¸€å±€æ¸¸æˆ

### ğŸ“ ä½ç½®ï¼š`alphazero/self_play.py`

```python
def play_game(self, temperature_schedule=None):
    """
    å®Œæ•´ä¸‹ä¸€å±€è‡ªæˆ‘å¯¹å¼ˆæ¸¸æˆ

    Returns:
        list: [(state, mcts_policy, value), ...]
    """
    if temperature_schedule is None:
        # é»˜è®¤æ¸©åº¦ç­–ç•¥ï¼šå‰30æ­¥æ¢ç´¢ï¼Œåç»­è´ªå©ª
        temperature_schedule = lambda move_num: 1.0 if move_num < 30 else 0.0

    train_examples = []
    game = self.game.clone()  # å¤åˆ¶æ¸¸æˆ
    game.reset()

    mcts = MCTS(self.network, game, self.args)
    move_count = 0

    while True:
        move_count += 1
        temperature = temperature_schedule(move_count)

        # === 1. è·å–å½“å‰çŠ¶æ€ï¼ˆè§„èŒƒåŒ–ï¼‰ ===
        canonical_board = game.get_canonical_board()
        # shape: (3, 5, 5)
        # æ€»æ˜¯ä»"å½“å‰ç©å®¶"è§†è§’

        # === 2. MCTSæœç´¢ ===
        action_probs = mcts.get_action_probs(game, temperature=temperature)
        # è¿è¡Œ200æ¬¡æ¨¡æ‹Ÿ
        # action_probs: [0.02, 0.01, 0.75, ..., 0.01]
        #                â†‘     â†‘     â†‘           â†‘
        #              ä½ç½®0  ä½ç½®1  ä½ç½®2(é«˜)  ä½ç½®24

        # === 3. è®°å½•è®­ç»ƒæ ·æœ¬ï¼ˆvalueæš‚æ—¶ä¸ºNoneï¼‰ ===
        train_examples.append([
            canonical_board,  # å½“å‰çŠ¶æ€
            action_probs,     # MCTSç»™å‡ºçš„ç­–ç•¥
            None              # ç»“æœï¼ˆæ¸¸æˆç»“æŸåå¡«ï¼‰
        ])

        # === 4. é€‰æ‹©åŠ¨ä½œ ===
        action = np.random.choice(len(action_probs), p=action_probs)
        # æ ¹æ®æ¦‚ç‡é‡‡æ ·ï¼šæ¦‚ç‡é«˜çš„ä½ç½®æ›´å¯èƒ½è¢«é€‰ä¸­

        # === 5. æ‰§è¡ŒåŠ¨ä½œ ===
        game.get_next_state(action)
        mcts.update_root(action)  # MCTSæ ‘é‡ç”¨

        # === 6. æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸ ===
        game_result = game.get_game_ended()
        if game_result != 0:
            # æ¸¸æˆç»“æŸï¼å¡«å……æ‰€æœ‰æ ·æœ¬çš„value
            return self._assign_rewards(train_examples, game_result)
```

#### ä»·å€¼åè½¬è¯¦è§£

```python
def _assign_rewards(self, train_examples, game_result):
    """
    Args:
        game_result: æœ€åä¸‹æ£‹è€…çš„ç»“æœ
                    +1 = èµ¢
                    -1 = è¾“
                     0 = å¹³å±€
    """
    completed_examples = []
    result = game_result

    # ä»åå¾€å‰å¡«
    for i in reversed(range(len(train_examples))):
        state, policy, _ = train_examples[i]
        completed_examples.append([state, policy, result])
        result = -result  # åè½¬ï¼

    completed_examples.reverse()
    return completed_examples
```

**ä¾‹å­**ï¼ˆ5æ­¥ç»“æŸçš„æ¸¸æˆï¼‰ï¼š

```
ç¬¬1æ­¥ï¼ˆé»‘æ–¹ä¸‹ï¼‰: è®°å½•[state1, policy1, None]
ç¬¬2æ­¥ï¼ˆç™½æ–¹ä¸‹ï¼‰: è®°å½•[state2, policy2, None]
ç¬¬3æ­¥ï¼ˆé»‘æ–¹ä¸‹ï¼‰: è®°å½•[state3, policy3, None]
ç¬¬4æ­¥ï¼ˆç™½æ–¹ä¸‹ï¼‰: è®°å½•[state4, policy4, None]
ç¬¬5æ­¥ï¼ˆé»‘æ–¹ä¸‹ï¼‰: é»‘æ–¹èƒœï¼game_result=+1

å¡«å……valueï¼ˆä»åå¾€å‰ï¼‰ï¼š
ç¬¬5æ­¥ï¼ˆé»‘æ–¹ï¼‰: value = +1ï¼ˆèµ¢ï¼‰
ç¬¬4æ­¥ï¼ˆç™½æ–¹ï¼‰: value = -1ï¼ˆè¾“ï¼‰
ç¬¬3æ­¥ï¼ˆé»‘æ–¹ï¼‰: value = +1ï¼ˆèµ¢ï¼‰
ç¬¬2æ­¥ï¼ˆç™½æ–¹ï¼‰: value = -1ï¼ˆè¾“ï¼‰
ç¬¬1æ­¥ï¼ˆé»‘æ–¹ï¼‰: value = +1ï¼ˆèµ¢ï¼‰

æœ€ç»ˆè®­ç»ƒæ•°æ®ï¼š
[
  (state1_é»‘, policy1, +1),  # é»‘æ–¹èµ¢
  (state2_ç™½, policy2, -1),  # ç™½æ–¹è¾“
  (state3_é»‘, policy3, +1),  # é»‘æ–¹èµ¢
  (state4_ç™½, policy4, -1),  # ç™½æ–¹è¾“
  (state5_é»‘, policy5, +1),  # é»‘æ–¹èµ¢
]
```

---

## ç¬¬å››éƒ¨åˆ†ï¼šè®­ç»ƒä¸€ä¸ªBatch

### ğŸ“ ä½ç½®ï¼š`alphazero/trainer.py`

```python
def _train_network(self):
    """è®­ç»ƒç¥ç»ç½‘ç»œ"""
    self.network.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼

    # === 1. åˆ›å»ºæ•°æ®åŠ è½½å™¨ ===
    dataset = TrainingDataset(list(self.train_examples_history))
    dataloader = DataLoader(
        dataset,
        batch_size=self.args.get('batch_size', 64),
        shuffle=True,  # æ‰“ä¹±æ•°æ®
        num_workers=0  # Windowså…¼å®¹æ€§
    )

    num_epochs = self.args.get('num_epochs', 10)
    total_loss = 0
    num_batches = 0

    # === 2. å¤šä¸ªepoch ===
    for epoch in range(num_epochs):
        epoch_loss = 0
        epoch_batches = 0

        # === 3. éå†æ¯ä¸ªbatch ===
        for states, target_policies, target_values in dataloader:
            # ç§»åŠ¨åˆ°è®¾å¤‡
            states = states.to(self.device)            # (batch, 3, 5, 5)
            target_policies = target_policies.to(self.device)  # (batch, 25)
            target_values = target_values.to(self.device)      # (batch, 1)

            # === 4. å‰å‘ä¼ æ’­ ===
            pred_policies, pred_values = self.network(states)
            # pred_policies: (batch, 25) logits
            # pred_values:   (batch, 1) é¢„æµ‹å€¼

            # === 5. è®¡ç®—æŸå¤± ===
            policy_loss = self._policy_loss(pred_policies, target_policies)
            value_loss = self._value_loss(pred_values, target_values)
            loss = policy_loss + value_loss

            # === 6. åå‘ä¼ æ’­ ===
            self.optimizer.zero_grad()  # æ¸…ç©ºä¹‹å‰çš„æ¢¯åº¦
            loss.backward()             # è®¡ç®—æ¢¯åº¦
            self.optimizer.step()       # æ›´æ–°å‚æ•°

            epoch_loss += loss.item()
            epoch_batches += 1

        avg_epoch_loss = epoch_loss / epoch_batches
        print(f"  Epoch {epoch + 1}/{num_epochs}: Loss = {avg_epoch_loss:.4f}")

        total_loss += epoch_loss
        num_batches += epoch_batches

    return total_loss / num_batches
```

#### æŸå¤±å‡½æ•°è¯¦è§£

```python
def _policy_loss(self, pred_policies, target_policies):
    """
    ç­–ç•¥æŸå¤±ï¼šäº¤å‰ç†µ

    Args:
        pred_policies: (batch, 25) logitsï¼ˆæœªå½’ä¸€åŒ–ï¼‰
        target_policies: (batch, 25) MCTSæ¦‚ç‡åˆ†å¸ƒ
    """
    # å…¬å¼ï¼š-Î£(target Ã— log(softmax(pred)))
    return -torch.sum(
        target_policies * torch.log_softmax(pred_policies, dim=1)
    ) / pred_policies.size(0)
    #     â†‘           â†‘                                         â†‘
    #   MCTSç­–ç•¥    ç½‘ç»œé¢„æµ‹çš„logæ¦‚ç‡                        å¹³å‡
```

**ä¾‹å­**ï¼š

```python
# å•ä¸ªæ ·æœ¬
target = [0.05, 0.02, 0.80, 0.10, ...]  # MCTSç­–ç•¥ï¼ˆä½ç½®2æ¦‚ç‡80%ï¼‰
pred_logits = [2.1, 1.5, 3.8, 2.0, ...] # ç½‘ç»œè¾“å‡º

# Softmax
pred_probs = softmax(pred_logits)
# = [0.18, 0.10, 0.64, 0.13, ...]       # ä½ç½®2æ¦‚ç‡64%ï¼ˆè¿˜ä¸å¤Ÿå¥½ï¼‰

# äº¤å‰ç†µ
loss = -Î£(target Ã— log(pred_probs))
     = -(0.05Ã—log(0.18) + 0.02Ã—log(0.10) + 0.80Ã—log(0.64) + ...)
     = çº¦0.6

# å¦‚æœç½‘ç»œå®Œå…¨åŒ¹é…MCTS:
perfect_probs = [0.05, 0.02, 0.80, 0.10, ...]
loss = -(0.80Ã—log(0.80) + ...) = çº¦0.3ï¼ˆæ›´ä½ï¼ï¼‰
```

```python
def _value_loss(self, pred_values, target_values):
    """
    ä»·å€¼æŸå¤±ï¼šå‡æ–¹è¯¯å·®

    Args:
        pred_values: (batch, 1) ç½‘ç»œé¢„æµ‹ [-1, 1]
        target_values: (batch, 1) çœŸå®ç»“æœ {-1, 0, +1}
    """
    return torch.mean((pred_values - target_values) ** 2)
```

**ä¾‹å­**ï¼š

```python
# Batch of 3
pred_values = [0.7, -0.5, 0.2]    # ç½‘ç»œé¢„æµ‹
target_values = [1.0, -1.0, 1.0]  # çœŸå®ç»“æœ

# MSE
loss = mean((0.7-1.0)Â² + (-0.5-(-1.0))Â² + (0.2-1.0)Â²)
     = mean(0.09 + 0.25 + 0.64)
     = 0.33

# å¦‚æœé¢„æµ‹å®Œç¾ï¼š
perfect_pred = [1.0, -1.0, 1.0]
loss = 0.0
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šå…³é”®æ•°æ®ç»“æ„

### 1. è®­ç»ƒæ ·æœ¬

```python
# å•ä¸ªæ ·æœ¬
sample = (state, policy, value)

# state: numpy.ndarray (3, 5, 5)
[
  [[0, 0, 1, ...],   # é€šé“0: å½“å‰ç©å®¶
   [0, 1, 0, ...],
   ...],
  [[1, 0, 0, ...],   # é€šé“1: å¯¹æ‰‹
   [0, 0, 1, ...],
   ...],
  [[1, 1, 1, ...],   # é€šé“2: å›åˆ
   [1, 1, 1, ...],
   ...]
]

# policy: numpy.ndarray (25,)
[0.02, 0.01, 0.75, 0.10, ...]  # MCTSè®¿é—®åˆ†å¸ƒ

# value: float
1.0  # å½“å‰ç©å®¶èµ¢
```

### 2. MCTSèŠ‚ç‚¹

```python
class MCTSNode:
    def __init__(self, prior_prob):
        self.visit_count = 0        # int: è®¿é—®æ¬¡æ•°
        self.total_value = 0.0      # float: ç´¯è®¡ä»·å€¼
        self.prior_prob = prior_prob # float: å…ˆéªŒæ¦‚ç‡ï¼ˆæ¥è‡ªç½‘ç»œï¼‰
        self.children = {}          # dict: {action: MCTSNode}
```

**æ ‘ç»“æ„ç¤ºä¾‹**ï¼š

```
æ ¹èŠ‚ç‚¹(N=200, V=30.5, P=0)
â”œâ”€â”€ åŠ¨ä½œ0: èŠ‚ç‚¹(N=80, V=48.2, P=0.35)
â”‚   â”œâ”€â”€ åŠ¨ä½œ5: èŠ‚ç‚¹(N=30, V=15.0, P=0.20)
â”‚   â”œâ”€â”€ åŠ¨ä½œ12: èŠ‚ç‚¹(N=40, V=28.0, P=0.40)
â”‚   â””â”€â”€ ...
â”œâ”€â”€ åŠ¨ä½œ1: èŠ‚ç‚¹(N=90, V=55.8, P=0.40)
â”œâ”€â”€ åŠ¨ä½œ2: èŠ‚ç‚¹(N=30, V=12.5, P=0.25)
â””â”€â”€ ...
```

### 3. æ¸¸æˆçŠ¶æ€

```python
class GomokuGame:
    def __init__(self, board_size, n_in_row):
        self.board = GomokuBoard(board_size, n_in_row)

# board.board: numpy.ndarray (15, 15)
# å€¼: 0=ç©º, 1=é»‘, -1=ç™½
[
  [0,  0,  1, 0, ...],
  [0, -1,  0, 1, ...],
  [1,  0,  0, 0, ...],
  ...
]

# board.current_player: int (1 or -1)
# board.last_move: tuple or None
# board.move_history: list of (row, col, player)
```

### 4. ç½‘ç»œå‚æ•°

```python
# 5Ã—5ç½‘ç»œå‚æ•°é‡
AlphaZeroNet(5, 64, 4).parameters()

# è®¡ç®—ï¼š
# Conv_input: 3Ã—64Ã—3Ã—3 = 1,728
# ResBlockÃ—4: æ¯ä¸ªçº¦ 64Ã—64Ã—3Ã—3Ã—2 = 73,728
# Policy head: çº¦ 10K
# Value head: çº¦ 5K
# æ€»è®¡: ~52Kå‚æ•°
```

---

## ğŸ“ ä»£ç é˜…è¯»å»ºè®®

### ä»å“ªé‡Œå¼€å§‹ï¼Ÿ

1. **æ–°æ‰‹**ï¼š
   - `gomoku/board.py` - æœ€ç®€å•çš„æ¸¸æˆé€»è¾‘
   - `alphazero/network.py` - çœ‹ç½‘ç»œç»“æ„
   - `train.py` - çœ‹ä¸»æµç¨‹

2. **è¿›é˜¶**ï¼š
   - `alphazero/mcts.py` - ç†è§£æœç´¢ç®—æ³•
   - `alphazero/self_play.py` - ç†è§£æ•°æ®ç”Ÿæˆ
   - `alphazero/trainer.py` - ç†è§£è®­ç»ƒè¿‡ç¨‹

3. **é«˜æ‰‹**ï¼š
   - ä¿®æ”¹UCBå…¬å¼ï¼Œè¯•è¯•ä¸åŒçš„æ¢ç´¢ç­–ç•¥
   - æ”¹è¿›ç½‘ç»œç»“æ„ï¼ŒåŠ å…¥attentionæœºåˆ¶
   - å®ç°å¹¶è¡ŒMCTS

### è°ƒè¯•æŠ€å·§

```python
# 1. æ‰“å°ä¸­é—´å€¼
def forward(self, x):
    print(f"Input shape: {x.shape}")
    out = self.conv1(x)
    print(f"After conv1: {out.shape}")
    ...

# 2. å¯è§†åŒ–MCTSæ ‘
def print_tree(node, depth=0):
    indent = "  " * depth
    print(f"{indent}N={node.visit_count}, V={node.value():.3f}")
    for action, child in node.children.items():
        print(f"{indent}Action {action}:")
        print_tree(child, depth+1)

# 3. æ£€æŸ¥æ¢¯åº¦
for name, param in network.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm={param.grad.norm().item():.4f}")
```

---

**ä¸‹ä¸€æ­¥**ï¼šå®Œæˆå®æˆ˜ç»ƒä¹  â†’ [06_å®æˆ˜ç»ƒä¹ .md](./06_å®æˆ˜ç»ƒä¹ .md)
