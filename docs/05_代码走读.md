# 05 - 代码走读

> 🎯 **学习目标**：深入理解关键代码的实现细节
> ⏱️ **预计时间**：60分钟
> 📌 **适合人群**：想深入理解代码的开发者

---

## 目录
1. [神经网络前向传播](#第一部分神经网络前向传播)
2. [MCTS一次模拟](#第二部分mcts一次模拟)
3. [自我对弈一局游戏](#第三部分自我对弈一局游戏)
4. [训练一个Batch](#第四部分训练一个batch)
5. [关键数据结构](#第五部分关键数据结构)

---

## 第一部分：神经网络前向传播

### 📍 位置：`alphazero/network.py`

#### 1. 输入数据准备

```python
def predict(self, board_state):
    """
    为单个棋盘状态预测策略和价值

    Args:
        board_state: shape (3, board_size, board_size)
                    通道0: 当前玩家棋子
                    通道1: 对手棋子
                    通道2: 回合指示器
    """
    self.eval()  # 切换到评估模式（关闭dropout等）
    with torch.no_grad():  # 不计算梯度（推理模式）
        if not isinstance(board_state, torch.Tensor):
            # 如果是numpy数组，转换为tensor
            board_state = torch.FloatTensor(board_state)

        # 添加batch维度: (3, 5, 5) -> (1, 3, 5, 5)
        if board_state.dim() == 3:
            board_state = board_state.unsqueeze(0)

        # 移动到正确的设备（CPU或GPU）
        board_state = board_state.to(next(self.parameters()).device)

        # 前向传播
        policy_logits, value = self.forward(board_state)

        # 转换policy logits为概率
        policy = F.softmax(policy_logits, dim=1)
        # softmax: [2.3, 1.5, 0.8, ...] -> [0.35, 0.15, 0.08, ...]
        #                                   (和为1的概率分布)

        # 转换为numpy并去掉batch维度
        policy = policy.cpu().numpy()[0]  # (25,)
        value = value.cpu().numpy()[0][0]  # 标量

    return policy, value
```

**关键点**：
- `eval()` + `torch.no_grad()`：推理模式，更快且省内存
- `unsqueeze(0)`：添加batch维度（网络期望batch输入）
- `softmax`：logits→概率分布

#### 2. 前向传播流程

```python
def forward(self, x):
    """
    Args:
        x: (batch, 3, board_size, board_size)

    Returns:
        policy_logits: (batch, board_size^2)
        value: (batch, 1)
    """
    # === 1. 初始卷积 ===
    out = self.conv_input(x)  # (batch, 3, 5, 5) -> (batch, 64, 5, 5)
    out = self.bn_input(out)  # Batch Normalization
    out = F.relu(out)         # 激活函数

    # === 2. 残差塔 ===
    for res_block in self.res_blocks:
        out = res_block(out)  # (batch, 64, 5, 5) -> (batch, 64, 5, 5)
        #                        通道数和尺寸不变，但特征更抽象

    # === 3. 策略头 ===
    policy = self.policy_conv(out)      # (batch, 64, 5, 5) -> (batch, 2, 5, 5)
    policy = self.policy_bn(policy)
    policy = F.relu(policy)
    policy = policy.view(policy.size(0), -1)  # 展平: (batch, 2×5×5) = (batch, 50)
    policy = self.policy_fc(policy)     # (batch, 50) -> (batch, 25)

    # === 4. 价值头 ===
    value = self.value_conv(out)        # (batch, 64, 5, 5) -> (batch, 1, 5, 5)
    value = self.value_bn(value)
    value = F.relu(value)
    value = value.view(value.size(0), -1)  # 展平: (batch, 25)
    value = self.value_fc1(value)       # (batch, 25) -> (batch, 256)
    value = F.relu(value)
    value = self.value_fc2(value)       # (batch, 256) -> (batch, 1)
    value = torch.tanh(value)           # 限制在[-1, 1]

    return policy, value
```

**数据形状变化**：

```
输入: (1, 3, 5, 5)
  ↓
初始卷积: (1, 64, 5, 5)
  ↓
残差块×4: (1, 64, 5, 5)  ← 形状不变，特征更深
  ↓          ↓
策略头     价值头
  ↓          ↓
(1, 25)   (1, 1)
```

#### 3. 残差块详解

```python
class ResidualBlock(nn.Module):
    def forward(self, x):
        residual = x  # 保存输入（跳跃连接）

        # 第一个卷积
        out = self.conv1(x)
        out = self.bn1(out)
        out = F.relu(out)

        # 第二个卷积
        out = self.conv2(out)
        out = self.bn2(out)

        # 关键：加上残差
        out += residual  # ← 这就是"残差连接"

        out = F.relu(out)
        return out
```

**为什么有效？**

```
普通网络：
  输入 → 卷积1 → 卷积2 → 输出
  如果网络很深，梯度会消失

残差网络：
  输入 → 卷积1 → 卷积2 → (+输入) → 输出
           ↓                   ↑
           └───────跳跃连接────┘

  梯度可以直接通过跳跃连接传回去！
```

---

## 第二部分：MCTS一次模拟

### 📍 位置：`alphazero/mcts.py`

#### 完整模拟流程

```python
def _simulate(self, game):
    """
    运行一次MCTS模拟

    Args:
        game: 当前游戏状态（会被修改，所以传入副本）

    Returns:
        float: 从根节点视角的价值估计
    """
    # === Step 0: 检查游戏是否已结束 ===
    game_ended = game.get_game_ended()
    if game_ended != 0:
        # 游戏结束，直接返回结果
        # 注意：返回负值，因为是从对手视角
        return -game_ended

    # === Step 1: 初始化根节点（如果需要） ===
    if self.root is None:
        # 第一次模拟，创建根节点
        policy, value = self.network.predict(game.get_canonical_board())

        # 屏蔽非法动作
        valid_moves = game.get_valid_moves()
        policy = policy * valid_moves  # 非法位置概率变0

        # 归一化
        policy_sum = np.sum(policy)
        if policy_sum > 0:
            policy = policy / policy_sum
        else:
            # 所有合法动作概率为0（罕见），用均匀分布
            policy = valid_moves / np.sum(valid_moves)

        # 创建根节点并扩展
        action_priors = {a: policy[a] for a in range(len(policy)) if valid_moves[a] > 0}
        self.root = MCTSNode(0)
        self.root.expand(action_priors)

        return -value  # 从对手视角返回

    # === Step 2: Selection（选择） ===
    node = self.root
    search_path = [node]  # 记录路径，用于反向传播
    action_path = []

    while not node.is_leaf():
        # 用UCB选择最佳子节点
        action, node = node.select_child(self.args['c_puct'])
        search_path.append(node)
        action_path.append(action)

        # 在游戏中执行这个动作
        game.get_next_state(action)

    # === Step 3: Expansion & Evaluation（扩展和评估） ===
    game_ended = game.get_game_ended()
    if game_ended != 0:
        # 到达终局，使用真实结果
        value = -game_ended
    else:
        # 叶子节点，用网络评估
        policy, value = self.network.predict(game.get_canonical_board())

        # 屏蔽非法动作
        valid_moves = game.get_valid_moves()
        policy = policy * valid_moves
        policy_sum = np.sum(policy)
        if policy_sum > 0:
            policy = policy / policy_sum
        else:
            policy = valid_moves / np.sum(valid_moves)

        # 扩展子节点
        action_priors = {a: policy[a] for a in range(len(policy)) if valid_moves[a] > 0}
        node.expand(action_priors)

    # === Step 4: Backpropagation（反向传播） ===
    for node in reversed(search_path):
        node.update(value)
        value = -value  # 每层反转（玩家切换）

    return value
```

**可视化一次模拟**：

```
初始状态（空棋盘）：
       根节点
       /  |  \
     25个子节点（都未访问）

第1次模拟：
  Selection: 根→选择prior最大的子节点A
  Expansion: 扩展A的子节点
  Evaluation: 网络评估value=+0.1
  Backpropagation:
    A: N=1, V=+0.1
    根: N=1, V=-0.1

第2次模拟：
  Selection: 根→再次选择（UCB重新计算）
  可能还是A（prior高），也可能是B（鼓励探索）
  ...

第200次模拟后：
       根节点(N=200)
       /   |    \
     A(90) B(80) C(30)  ← 访问次数
```

#### UCB选择详解

```python
def get_ucb_score(self, parent_visit_count, c_puct):
    """
    UCB = Q + U
    Q = exploitation（利用）= 平均价值
    U = exploration（探索）= 鼓励访问少的节点
    """
    # Q: 平均价值
    q_value = self.value()  # = total_value / visit_count

    # U: 探索奖励
    u_value = c_puct * self.prior_prob * math.sqrt(parent_visit_count) / (1 + self.visit_count)
    #         ↑      ↑                  ↑                                  ↑
    #       常数   网络建议           父节点越多访问                   自己访问少→U大
    #       (1.0)  (policy)          →鼓励探索                        (鼓励探索)

    return q_value + u_value
```

**例子**：

```python
# 父节点访问了100次
parent_N = 100
c_puct = 1.0

# 节点A: 访问多，Q高
A.visit_count = 50
A.prior_prob = 0.2
A.total_value = 30
Q_A = 30/50 = 0.6
U_A = 1.0 × 0.2 × √100 / (1+50) = 0.039
UCB_A = 0.6 + 0.039 = 0.639

# 节点B: 访问少，Q低
B.visit_count = 5
B.prior_prob = 0.1
B.total_value = 2
Q_B = 2/5 = 0.4
U_B = 1.0 × 0.1 × √100 / (1+5) = 0.167
UCB_B = 0.4 + 0.167 = 0.567

# 节点C: 未访问，prior高
C.visit_count = 0
C.prior_prob = 0.5
Q_C = 0
U_C = 1.0 × 0.5 × √100 / (1+0) = 5.0
UCB_C = 0 + 5.0 = 5.0  ← 最高！会被选择

# 结论：未访问且网络认为好的节点会被优先探索
```

---

## 第三部分：自我对弈一局游戏

### 📍 位置：`alphazero/self_play.py`

```python
def play_game(self, temperature_schedule=None):
    """
    完整下一局自我对弈游戏

    Returns:
        list: [(state, mcts_policy, value), ...]
    """
    if temperature_schedule is None:
        # 默认温度策略：前30步探索，后续贪婪
        temperature_schedule = lambda move_num: 1.0 if move_num < 30 else 0.0

    train_examples = []
    game = self.game.clone()  # 复制游戏
    game.reset()

    mcts = MCTS(self.network, game, self.args)
    move_count = 0

    while True:
        move_count += 1
        temperature = temperature_schedule(move_count)

        # === 1. 获取当前状态（规范化） ===
        canonical_board = game.get_canonical_board()
        # shape: (3, 5, 5)
        # 总是从"当前玩家"视角

        # === 2. MCTS搜索 ===
        action_probs = mcts.get_action_probs(game, temperature=temperature)
        # 运行200次模拟
        # action_probs: [0.02, 0.01, 0.75, ..., 0.01]
        #                ↑     ↑     ↑           ↑
        #              位置0  位置1  位置2(高)  位置24

        # === 3. 记录训练样本（value暂时为None） ===
        train_examples.append([
            canonical_board,  # 当前状态
            action_probs,     # MCTS给出的策略
            None              # 结果（游戏结束后填）
        ])

        # === 4. 选择动作 ===
        action = np.random.choice(len(action_probs), p=action_probs)
        # 根据概率采样：概率高的位置更可能被选中

        # === 5. 执行动作 ===
        game.get_next_state(action)
        mcts.update_root(action)  # MCTS树重用

        # === 6. 检查游戏是否结束 ===
        game_result = game.get_game_ended()
        if game_result != 0:
            # 游戏结束！填充所有样本的value
            return self._assign_rewards(train_examples, game_result)
```

#### 价值反转详解

```python
def _assign_rewards(self, train_examples, game_result):
    """
    Args:
        game_result: 最后下棋者的结果
                    +1 = 赢
                    -1 = 输
                     0 = 平局
    """
    completed_examples = []
    result = game_result

    # 从后往前填
    for i in reversed(range(len(train_examples))):
        state, policy, _ = train_examples[i]
        completed_examples.append([state, policy, result])
        result = -result  # 反转！

    completed_examples.reverse()
    return completed_examples
```

**例子**（5步结束的游戏）：

```
第1步（黑方下）: 记录[state1, policy1, None]
第2步（白方下）: 记录[state2, policy2, None]
第3步（黑方下）: 记录[state3, policy3, None]
第4步（白方下）: 记录[state4, policy4, None]
第5步（黑方下）: 黑方胜！game_result=+1

填充value（从后往前）：
第5步（黑方）: value = +1（赢）
第4步（白方）: value = -1（输）
第3步（黑方）: value = +1（赢）
第2步（白方）: value = -1（输）
第1步（黑方）: value = +1（赢）

最终训练数据：
[
  (state1_黑, policy1, +1),  # 黑方赢
  (state2_白, policy2, -1),  # 白方输
  (state3_黑, policy3, +1),  # 黑方赢
  (state4_白, policy4, -1),  # 白方输
  (state5_黑, policy5, +1),  # 黑方赢
]
```

---

## 第四部分：训练一个Batch

### 📍 位置：`alphazero/trainer.py`

```python
def _train_network(self):
    """训练神经网络"""
    self.network.train()  # 切换到训练模式

    # === 1. 创建数据加载器 ===
    dataset = TrainingDataset(list(self.train_examples_history))
    dataloader = DataLoader(
        dataset,
        batch_size=self.args.get('batch_size', 64),
        shuffle=True,  # 打乱数据
        num_workers=0  # Windows兼容性
    )

    num_epochs = self.args.get('num_epochs', 10)
    total_loss = 0
    num_batches = 0

    # === 2. 多个epoch ===
    for epoch in range(num_epochs):
        epoch_loss = 0
        epoch_batches = 0

        # === 3. 遍历每个batch ===
        for states, target_policies, target_values in dataloader:
            # 移动到设备
            states = states.to(self.device)            # (batch, 3, 5, 5)
            target_policies = target_policies.to(self.device)  # (batch, 25)
            target_values = target_values.to(self.device)      # (batch, 1)

            # === 4. 前向传播 ===
            pred_policies, pred_values = self.network(states)
            # pred_policies: (batch, 25) logits
            # pred_values:   (batch, 1) 预测值

            # === 5. 计算损失 ===
            policy_loss = self._policy_loss(pred_policies, target_policies)
            value_loss = self._value_loss(pred_values, target_values)
            loss = policy_loss + value_loss

            # === 6. 反向传播 ===
            self.optimizer.zero_grad()  # 清空之前的梯度
            loss.backward()             # 计算梯度
            self.optimizer.step()       # 更新参数

            epoch_loss += loss.item()
            epoch_batches += 1

        avg_epoch_loss = epoch_loss / epoch_batches
        print(f"  Epoch {epoch + 1}/{num_epochs}: Loss = {avg_epoch_loss:.4f}")

        total_loss += epoch_loss
        num_batches += epoch_batches

    return total_loss / num_batches
```

#### 损失函数详解

```python
def _policy_loss(self, pred_policies, target_policies):
    """
    策略损失：交叉熵

    Args:
        pred_policies: (batch, 25) logits（未归一化）
        target_policies: (batch, 25) MCTS概率分布
    """
    # 公式：-Σ(target × log(softmax(pred)))
    return -torch.sum(
        target_policies * torch.log_softmax(pred_policies, dim=1)
    ) / pred_policies.size(0)
    #     ↑           ↑                                         ↑
    #   MCTS策略    网络预测的log概率                        平均
```

**例子**：

```python
# 单个样本
target = [0.05, 0.02, 0.80, 0.10, ...]  # MCTS策略（位置2概率80%）
pred_logits = [2.1, 1.5, 3.8, 2.0, ...] # 网络输出

# Softmax
pred_probs = softmax(pred_logits)
# = [0.18, 0.10, 0.64, 0.13, ...]       # 位置2概率64%（还不够好）

# 交叉熵
loss = -Σ(target × log(pred_probs))
     = -(0.05×log(0.18) + 0.02×log(0.10) + 0.80×log(0.64) + ...)
     = 约0.6

# 如果网络完全匹配MCTS:
perfect_probs = [0.05, 0.02, 0.80, 0.10, ...]
loss = -(0.80×log(0.80) + ...) = 约0.3（更低！）
```

```python
def _value_loss(self, pred_values, target_values):
    """
    价值损失：均方误差

    Args:
        pred_values: (batch, 1) 网络预测 [-1, 1]
        target_values: (batch, 1) 真实结果 {-1, 0, +1}
    """
    return torch.mean((pred_values - target_values) ** 2)
```

**例子**：

```python
# Batch of 3
pred_values = [0.7, -0.5, 0.2]    # 网络预测
target_values = [1.0, -1.0, 1.0]  # 真实结果

# MSE
loss = mean((0.7-1.0)² + (-0.5-(-1.0))² + (0.2-1.0)²)
     = mean(0.09 + 0.25 + 0.64)
     = 0.33

# 如果预测完美：
perfect_pred = [1.0, -1.0, 1.0]
loss = 0.0
```

---

## 第五部分：关键数据结构

### 1. 训练样本

```python
# 单个样本
sample = (state, policy, value)

# state: numpy.ndarray (3, 5, 5)
[
  [[0, 0, 1, ...],   # 通道0: 当前玩家
   [0, 1, 0, ...],
   ...],
  [[1, 0, 0, ...],   # 通道1: 对手
   [0, 0, 1, ...],
   ...],
  [[1, 1, 1, ...],   # 通道2: 回合
   [1, 1, 1, ...],
   ...]
]

# policy: numpy.ndarray (25,)
[0.02, 0.01, 0.75, 0.10, ...]  # MCTS访问分布

# value: float
1.0  # 当前玩家赢
```

### 2. MCTS节点

```python
class MCTSNode:
    def __init__(self, prior_prob):
        self.visit_count = 0        # int: 访问次数
        self.total_value = 0.0      # float: 累计价值
        self.prior_prob = prior_prob # float: 先验概率（来自网络）
        self.children = {}          # dict: {action: MCTSNode}
```

**树结构示例**：

```
根节点(N=200, V=30.5, P=0)
├── 动作0: 节点(N=80, V=48.2, P=0.35)
│   ├── 动作5: 节点(N=30, V=15.0, P=0.20)
│   ├── 动作12: 节点(N=40, V=28.0, P=0.40)
│   └── ...
├── 动作1: 节点(N=90, V=55.8, P=0.40)
├── 动作2: 节点(N=30, V=12.5, P=0.25)
└── ...
```

### 3. 游戏状态

```python
class GomokuGame:
    def __init__(self, board_size, n_in_row):
        self.board = GomokuBoard(board_size, n_in_row)

# board.board: numpy.ndarray (15, 15)
# 值: 0=空, 1=黑, -1=白
[
  [0,  0,  1, 0, ...],
  [0, -1,  0, 1, ...],
  [1,  0,  0, 0, ...],
  ...
]

# board.current_player: int (1 or -1)
# board.last_move: tuple or None
# board.move_history: list of (row, col, player)
```

### 4. 网络参数

```python
# 5×5网络参数量
AlphaZeroNet(5, 64, 4).parameters()

# 计算：
# Conv_input: 3×64×3×3 = 1,728
# ResBlock×4: 每个约 64×64×3×3×2 = 73,728
# Policy head: 约 10K
# Value head: 约 5K
# 总计: ~52K参数
```

---

## 🎓 代码阅读建议

### 从哪里开始？

1. **新手**：
   - `gomoku/board.py` - 最简单的游戏逻辑
   - `alphazero/network.py` - 看网络结构
   - `train.py` - 看主流程

2. **进阶**：
   - `alphazero/mcts.py` - 理解搜索算法
   - `alphazero/self_play.py` - 理解数据生成
   - `alphazero/trainer.py` - 理解训练过程

3. **高手**：
   - 修改UCB公式，试试不同的探索策略
   - 改进网络结构，加入attention机制
   - 实现并行MCTS

### 调试技巧

```python
# 1. 打印中间值
def forward(self, x):
    print(f"Input shape: {x.shape}")
    out = self.conv1(x)
    print(f"After conv1: {out.shape}")
    ...

# 2. 可视化MCTS树
def print_tree(node, depth=0):
    indent = "  " * depth
    print(f"{indent}N={node.visit_count}, V={node.value():.3f}")
    for action, child in node.children.items():
        print(f"{indent}Action {action}:")
        print_tree(child, depth+1)

# 3. 检查梯度
for name, param in network.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm={param.grad.norm().item():.4f}")
```

---

**下一步**：完成实战练习 → [06_实战练习.md](./06_实战练习.md)
