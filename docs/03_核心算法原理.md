# 03 - 核心算法原理

> 🎯 **学习目标**：深入理解AlphaZero的工作原理
> ⏱️ **预计时间**：45分钟
> 📌 **适合人群**：想理解算法细节的学习者

---

## 目录
1. [AlphaZero完整训练循环](#第一部分alphazero完整训练循环)
2. [MCTS蒙特卡洛树搜索](#第二部分mcts蒙特卡洛树搜索)
3. [ResNet神经网络](#第三部分resnet神经网络)
4. [自我对弈机制](#第四部分自我对弈机制)
5. [训练过程](#第五部分训练过程)
6. [为什么AlphaZero有效](#第六部分为什么alphazero有效)

---

## 第一部分：AlphaZero完整训练循环

### 🔄 核心循环图

```
┌─────────────────────────────────────────────┐
│         AlphaZero 训练循环                    │
└─────────────────────────────────────────────┘
                    │
                    ↓
        ┌───────────────────────┐
        │  1. 自我对弈 (Self-Play) │
        │     - 用当前网络 + MCTS   │
        │     - 下N局游戏          │
        │     - 收集训练数据        │
        └───────────────────────┘
                    │
                    ↓
        ┌───────────────────────┐
        │  2. 训练网络 (Train)     │
        │     - 用对弈数据         │
        │     - 优化策略 + 价值网络 │
        │     - 减小Loss           │
        └───────────────────────┘
                    │
                    ↓
        ┌───────────────────────┐
        │  3. 评估 (Evaluate)     │
        │     - 测试新网络实力     │
        │     - 保存更好的模型     │
        └───────────────────────┘
                    │
                    ↓
        网络变强了！继续循环...
```

### 📋 详细步骤

#### 初始化阶段（第0轮）
```python
# 1. 创建神经网络（随机初始化）
network = AlphaZeroNet(board_size=5)
# 此时网络什么都不会，输出接近随机

# 2. 准备空的训练数据缓存
training_buffer = []
```

#### 迭代训练（第1-100轮）

**每一轮做三件事**：

```python
for iteration in range(1, 101):
    # === 步骤1: 自我对弈 ===
    games_data = []
    for game_num in range(50):  # 50局游戏
        data = self_play_one_game(network)
        # data = [(state1, policy1, value1),
        #         (state2, policy2, value2), ...]
        games_data.extend(data)

    # === 步骤2: 训练网络 ===
    for epoch in range(5):  # 训练5个epoch
        train_network(network, games_data)
        # 让网络学习模仿MCTS的决策

    # === 步骤3: 保存模型 ===
    save_checkpoint(network, iteration)

    # 网络进步了，下一轮用更强的网络继续对弈！
```

### 🎯 关键洞察

**为什么循环会让AI变强？**

1. **初期**（迭代1-10）
   - 网络随机，MCTS帮助找到合理走法
   - 对弈质量低，但总比纯随机好

2. **中期**（迭代20-50）
   - 网络学会基本模式（角、边、中心）
   - MCTS基于更好的网络，找到更好的走法
   - 对弈质量提升 → 训练数据更好 → 网络更强

3. **后期**（迭代80-100）
   - 网络已经很强，MCTS微调
   - 发现高深策略
   - 达到超人水平

**正反馈循环**：
```
更好的网络 → 更好的MCTS → 更好的训练数据
     ↑                                    ↓
     └────────────────────────────────────┘
```

---

## 第二部分：MCTS蒙特卡洛树搜索

### 🌲 什么是MCTS？

**Monte Carlo Tree Search** = 蒙特卡洛树搜索

**核心思想**：模拟未来多步，找出最有希望的走法

**比喻**：像规划旅行路线
- 你要从北京去上海，有多种路线
- 你不会真的走每条路，而是"在脑海中模拟"
- 哪条路看起来不错？多模拟几次
- 最后选择模拟中最常走的那条路

### 🎯 MCTS的四个步骤（核心！）

#### 步骤1：Selection（选择）

**目标**：从根节点沿着树向下，选择最值得探索的子节点

**使用UCB公式**：
```python
def get_ucb_score(self, parent_N, c_puct):
    Q = self.total_value / self.visit_count  # 平均价值
    U = c_puct * P * sqrt(parent_N) / (1 + self.visit_count)  # 探索奖励
    return Q + U
```

**公式解释**：
- **Q（质量）**：这个节点历史上的平均表现
  - 高Q = 之前模拟时，这条路通常赢
  - 低Q = 之前模拟时，这条路通常输

- **U（不确定性）**：探索奖励
  - 访问少的节点U大（鼓励探索）
  - 访问多的节点U小（减少探索）

- **P（先验）**：神经网络的建议
  - 网络认为这步好，P大
  - 网络认为这步差，P小

**选择策略**：
```
每次选择 UCB = Q + U 最高的子节点

例子：
节点A: Q=0.7, U=0.1 → UCB=0.8
节点B: Q=0.5, U=0.6 → UCB=1.1 ← 选这个！
节点C: Q=0.8, U=0.0 → UCB=0.8

虽然C的Q最高，但B访问少，U大，总分最高
```

**图示**：
```
           根节点
          /   |   \
         /    |    \
      节点A  节点B  节点C
      Q=0.7  Q=0.5  Q=0.8
      U=0.1  U=0.6  U=0.0
      ↓      ↓      ↓
      0.8    1.1←选择  0.8
```

#### 步骤2：Expansion（扩展）

**目标**：到达叶子节点后，扩展所有可能的下一步

**过程**：
```python
# 假设到达一个叶子节点（当前棋盘状态）
if node.is_leaf():
    # 用神经网络评估这个局面
    policy, value = network.predict(board_state)

    # 根据policy创建子节点
    for action in valid_actions:
        prior_prob = policy[action]
        child = MCTSNode(prior_prob)
        node.children[action] = child
```

**例子**：
```
当前棋盘（5×5）：
X . . . .
. O . . .
. . X . .
. . . O .
. . . . .

合法位置有21个空位
→ 创建21个子节点
→ 每个子节点的prior_prob来自网络的policy输出
```

#### 步骤3：Evaluation（评估）

**目标**：用神经网络评估叶子节点的好坏

**代码**：
```python
policy, value = network.predict(board_state)

# policy: [0.05, 0.02, 0.15, ..., 0.08]  ← 25个位置的概率
# value:  0.65                           ← 当前玩家胜率65%
```

**网络回答两个问题**：
1. **Policy**: 从这个局面，哪些位置看起来不错？
2. **Value**: 从这个局面，当前玩家优势如何？

**注意**：
- 网络不是"下一步应该走哪"
- 而是"这个局面对当前玩家好不好"

#### 步骤4：Backpropagation（反向传播）

**目标**：把评估结果反向更新到路径上所有节点

**过程**：
```python
value = 0.65  # 从评估得到的价值

# 沿着搜索路径向上回溯
for node in reversed(search_path):
    node.visit_count += 1
    node.total_value += value
    value = -value  # 反转！因为玩家切换了
```

**为什么要反转价值？**

```
假设：
第1步（黑方下）: value = +0.6（黑方优势）
↓
第2步（白方下）: value = -0.6（白方劣势 = 黑方优势）
↓
第3步（黑方下）: value = +0.6（黑方优势）

价值是"相对于当前玩家"的！
```

**图示**：
```
开始模拟：
    根(黑)
     ↓ 选择A
    节点A(白)
     ↓ 选择B
    节点B(黑) → 评估value=+0.7（黑方好）

反向传播：
    节点B: N=1, V=+0.7
      ↓
    节点A: N=1, V=-0.7（白方差）
      ↓
    根节点: N=1, V=+0.7（黑方好）
```

### 🔁 完整MCTS流程示例

**场景**：5×5五子棋，AI执黑，当前空棋盘

```python
# === 第1次模拟 ===
1. Selection: 从根出发（空棋盘）
   → 根节点是叶子，跳到Expansion

2. Expansion: 扩展25个子节点（25个位置）
   用网络评估：policy=[0.01, 0.01, 0.35, ..., 0.01]
                                    ↑ 中心位置概率高
   创建25个子节点

3. Evaluation: value=0.0（初始局面，势均力敌）

4. Backpropagation: 更新根节点 N=1, V=0.0

# === 第2次模拟 ===
1. Selection: 从根出发
   → 有25个子节点，都N=0
   → 选择prior_prob最大的（中心位置）

2. Expansion: 下在中心后，扩展24个子节点

3. Evaluation: value=+0.1（中心稍占优）

4. Backpropagation:
   中心节点: N=1, V=+0.1
   根节点:   N=2, V=+0.1

# === 重复200次模拟 ===
...

# === 最终决策 ===
根节点的子节点访问统计：
位置(0,0): N=2
位置(1,1): N=5
位置(2,2): N=180 ← 中心位置访问最多！
位置(3,3): N=10
...

→ 选择访问次数最多的位置：(2,2)中心
```

### 🎲 温度（Temperature）参数

**控制随机性**：

```python
if temperature == 0:
    # 贪婪：只选访问最多的
    action = argmax(visit_counts)

else:
    # 随机采样：按访问次数的概率分布
    visits_with_temp = visit_counts ** (1/temperature)
    probs = visits_with_temp / sum(visits_with_temp)
    action = random.choice(actions, p=probs)
```

**温度的作用**：

| Temperature | 行为 | 用于 |
|------------|------|-----|
| T = 0 | 完全贪婪，选最优 | 实战对弈 |
| T = 1 | 随机探索，多样性 | 训练前期 |
| T → ∞ | 完全随机 | 不推荐 |

**训练策略**：
```python
def temperature_schedule(move_number):
    if move_number < 30:
        return 1.0  # 前30步探索
    else:
        return 0.0  # 后续选最优
```

---

## 第三部分：ResNet神经网络

### 🧠 网络架构

#### 整体结构

```
输入: (3, 15, 15)
  ├─ 通道1: 当前玩家的棋子
  ├─ 通道2: 对手的棋子
  └─ 通道3: 回合标记（全0或全1）

  ↓

[卷积层] Conv(3 → 256, 3×3) + BN + ReLU

  ↓

[残差块] × 10层
每层: Conv(256→256) + BN + ReLU
     + Conv(256→256) + BN
     + Skip Connection
     + ReLU

  ↓           ↓

策略头        价值头
  ↓           ↓

Policy      Value
(225,)      [-1, 1]
```

#### 为什么用ResNet？

**问题**：普通深层网络训练困难
- 层数多→梯度消失
- 无法优化很深的网络

**解决**：残差连接（Skip Connection）

```python
# 普通卷积块
out = Conv2(ReLU(Conv1(x)))

# 残差块
out = ReLU(Conv2(ReLU(Conv1(x))) + x)
                                 ↑ 跳跃连接！
```

**好处**：
- ✅ 梯度可以直接传到前面层
- ✅ 可以堆很深（20层+）
- ✅ 更强大的特征提取

**图示**：
```
输入x
  ├────────────┐ 跳跃连接
  ↓            │
Conv1          │
  ↓            │
ReLU           │
  ↓            │
Conv2          │
  ↓            │
BN             │
  ↓            │
相加 ←─────────┘
  ↓
ReLU
  ↓
输出
```

### 📊 双头设计

AlphaZero的核心创新：**一个网络，两个输出**

#### 头1：策略头（Policy Head）

**目标**：预测每个位置的"好坏"概率

```python
# 从共享层出来的特征
features = (batch, 256, 15, 15)

# 策略头
policy = Conv(256 → 2, 1×1)     # 降维到2通道
policy = BN + ReLU
policy = Flatten                 # (batch, 2×15×15)
policy = Linear(450 → 225)       # 输出每个位置
policy = Softmax                 # 转换为概率分布

# 输出
policy_probs = [0.002, 0.001, 0.15, ..., 0.003]
               ↑      ↑      ↑           ↑
             位置0   位置1  位置2(高)   位置224
```

**损失函数**：交叉熵
```python
policy_loss = -Σ(mcts_policy × log(network_policy))

# 让网络的输出接近MCTS的访问分布
```

#### 头2：价值头（Value Head）

**目标**：预测当前局面的胜率

```python
# 价值头
value = Conv(256 → 1, 1×1)      # 降维到1通道
value = BN + ReLU
value = Flatten                  # (batch, 15×15)
value = Linear(225 → 256)        # 隐藏层
value = ReLU
value = Linear(256 → 1)          # 输出1个值
value = Tanh                     # 限制在[-1, 1]

# 输出
value = 0.67  # 当前玩家有67%胜率（优势）
```

**损失函数**：均方误差
```python
value_loss = (network_value - game_outcome)²

# 让网络的预测接近真实结果
# game_outcome ∈ {-1, 0, +1}
```

### 🎨 特征平面设计

**输入的3个通道含义**：

```python
state = game.get_feature_planes()
# state.shape = (3, 15, 15)

# 通道0: 当前玩家的棋子
[
  [0, 0, 0, ..., 0],
  [0, 1, 0, ..., 0],  ← 这里有当前玩家的子
  [0, 0, 1, ..., 0],
  ...
]

# 通道1: 对手的棋子
[
  [0, 0, 0, ..., 0],
  [0, 0, 1, ..., 0],  ← 这里有对手的子
  [0, 0, 0, ..., 0],
  ...
]

# 通道2: 回合标记
[
  [1, 1, 1, ..., 1],
  [1, 1, 1, ..., 1],  ← 全1表示黑方回合
  ...
]
# 或全0表示白方回合
```

**为什么这样设计？**
- ✅ 旋转棋盘后特征一致（对称性）
- ✅ 网络只需学一个策略（当前玩家视角）
- ✅ 简化训练

---

## 第四部分：自我对弈机制

### 🎮 一局自我对弈的完整流程

```python
def self_play_one_game():
    game.reset()
    train_examples = []

    move_num = 0
    while True:
        move_num += 1

        # 1. 获取当前棋盘（规范化到当前玩家视角）
        canonical_board = game.get_canonical_board()

        # 2. MCTS搜索（200次模拟）
        mcts = MCTS(network, game, args)
        temp = 1.0 if move_num < 30 else 0.0
        action_probs = mcts.get_action_probs(game, temperature=temp)

        # 3. 记录训练样本（暂时不知道结果）
        train_examples.append([
            canonical_board,   # 状态
            action_probs,      # MCTS策略
            None               # 结果（稍后填充）
        ])

        # 4. 选择动作并执行
        action = np.random.choice(len(action_probs), p=action_probs)
        game.make_move(action)

        # 5. 检查游戏是否结束
        game_result = game.get_game_ended()
        if game_result != 0:
            # 游戏结束，填充所有样本的结果
            return assign_values(train_examples, game_result)
```

### 📝 训练样本的形式

```python
# 一局游戏产生多个样本
game_samples = [
    (state_1, mcts_policy_1, +1),  # 第1步，最终赢了
    (state_2, mcts_policy_2, -1),  # 第2步，对手视角输了
    (state_3, mcts_policy_3, +1),  # 第3步，又赢了
    ...
    (state_50, mcts_policy_50, +1), # 最后一步，赢了
]

# 注意：value是相对于当前玩家的
# 黑方视角: +1 = 黑赢
# 白方视角: +1 = 白赢
```

### 🔄 价值反转

**关键**：填充结果时要反转价值

```python
def assign_rewards(examples, final_result):
    """
    final_result: 从最后下棋者的视角
    +1 = 最后下棋的人赢了
    -1 = 最后下棋的人输了
    """
    completed_examples = []
    result = final_result

    # 从后往前填充
    for i in reversed(range(len(examples))):
        state, policy, _ = examples[i]
        completed_examples.append((state, policy, result))
        result = -result  # 反转！因为玩家切换

    completed_examples.reverse()
    return completed_examples
```

**例子**：
```
游戏结束，黑方胜（最后是黑方下的）

倒序填充：
第50步（黑方）: value = +1（赢）
第49步（白方）: value = -1（输 = 对手赢）
第48步（黑方）: value = +1（赢）
第47步（白方）: value = -1（输）
...
第1步（黑方）:  value = +1（赢）
```

### 🎲 数据增强

**利用对称性增加8倍数据！**

```python
def get_symmetries(board, policy):
    # 5×5棋盘的8种对称
    symmetries = []

    # 原始
    symmetries.append((board, policy))

    # 旋转90°
    board_90 = rotate(board, 90)
    policy_90 = rotate(policy, 90)
    symmetries.append((board_90, policy_90))

    # 旋转180°
    ...

    # 旋转270°
    ...

    # 水平翻转
    ...

    # 水平翻转 + 旋转90/180/270
    ...

    return symmetries  # 8个样本
```

**为什么有效？**
- 五子棋的策略对旋转/翻转不变
- 原始：中心是好位置
- 旋转后：中心还是好位置
- 网络学到的是"相对模式"

**数据量提升**：
```
50局游戏 × 平均30步/局 × 8种对称
= 12,000个训练样本！
```

---

## 第五部分：训练过程

### 📚 训练数据管理

#### 数据缓冲区

```python
class Trainer:
    def __init__(self, ...):
        # 双端队列，自动保留最近的数据
        self.train_examples_history = deque(
            maxlen=200000  # 最多20万个样本
        )
```

**为什么用deque？**
- 自动淘汰旧数据
- 总是训练最近的对弈数据
- 随着网络进步，数据质量提升

#### 训练流程

```python
for iteration in range(1, 101):
    # 1. 自我对弈生成数据
    new_data = self_play(network, 50 games)
    # 约50局 × 30步 × 8对称 = 12,000样本

    # 2. 添加到缓冲区
    buffer.extend(new_data)
    # buffer现在有 12,000 → 24,000 → ... → 200,000

    # 3. 从缓冲区采样训练
    for epoch in range(10):
        batches = sample_batches(buffer, batch_size=64)
        for batch in batches:
            loss = train_one_batch(batch)
```

### 📉 损失函数详解

#### 总损失 = 策略损失 + 价值损失

```python
def compute_loss(network_output, targets):
    policy_logits, value_pred = network_output
    target_policy, target_value = targets

    # 策略损失：交叉熵
    policy_loss = cross_entropy(
        target_policy,   # MCTS访问分布 [0.05, 0.02, 0.80, ...]
        policy_logits    # 网络输出logits
    )

    # 价值损失：均方误差
    value_loss = mse(
        target_value,    # 真实结果 +1/-1
        value_pred       # 网络预测 0.67
    )

    total_loss = policy_loss + value_loss
    return total_loss
```

#### 损失下降的意义

```
Iteration 1:  Loss = 2.5  ← 网络随机，损失很大
Iteration 10: Loss = 1.8  ← 开始学习基本模式
Iteration 30: Loss = 1.2  ← 学会了策略
Iteration 100: Loss = 0.8 ← 收敛，很强了
```

**健康的训练曲线**：
```
Loss
 ↑
2.5 |■
2.0 | ■
1.5 |  ■■
1.0 |     ■■■■
0.5 |          ■■■■■■
0.0 |___________________→ Iteration
    0  20  40  60  80 100
```

### 🔧 优化器和学习率

```python
optimizer = torch.optim.Adam(
    network.parameters(),
    lr=0.001,          # 学习率
    weight_decay=1e-4  # L2正则化，防止过拟合
)
```

**Adam优化器的优势**：
- 自适应学习率
- 对超参数不敏感
- 训练稳定

---

## 第六部分：为什么AlphaZero有效？

### 💡 核心创新点

#### 1. 搜索与学习的结合

**传统MCTS**：
- ✅ 搜索能力强
- ❌ 需要大量计算
- ❌ 没有知识积累

**深度学习**：
- ✅ 学习模式识别
- ❌ 缺乏搜索规划
- ❌ 对复杂局面无能为力

**AlphaZero**：
- ✅ MCTS提供短期战术
- ✅ 神经网络提供长期战略
- ✅ 互相增强

```
第1轮:
  MCTS(弱网络) → 中等质量数据 → 训练 → 稍好的网络

第10轮:
  MCTS(好网络) → 高质量数据 → 训练 → 更好的网络

第100轮:
  MCTS(强网络) → 极高质量数据 → 训练 → 超强网络
```

#### 2. 自我对弈的威力

**问题**：人类棋谱有限，且有偏见

**解决**：AI自己创造数据
- 无限数据来源
- 对手水平随自己增长
- 发现新招法

**对比**：

| 方法 | 数据来源 | 对手水平 | 创新能力 |
|------|---------|---------|---------|
| 监督学习 | 人类棋谱 | 固定（人类） | 低 |
| vs随机 | 自我生成 | 太弱 | 低 |
| vs自己 | 自我生成 | 动态提升 | 高！|

#### 3. 双头网络的优势

**策略头**：学习"专家移动"
**价值头**：学习"局面评估"

两个任务互相帮助：
- 好的策略 → 准确的价值
- 准确的价值 → 更好的策略

### 🚀 收敛速度

**为什么AlphaZero能快速学习？**

1. **高质量标签**
   - MCTS搜索的结果比随机好得多
   - 相当于有"半专家"教学

2. **正反馈**
   - 每轮都在进步
   - 进步速度加快

3. **数据增强**
   - 8倍对称扩展
   - 有效样本更多

**学习曲线示意**：
```
实力
 ↑
超|              ╱
人|            ╱
级|          ╱
 |        ╱
强|      ╱
 |    ╱
中|  ╱
 |╱
弱|___________________→ Iteration
  0  20  40  60  80 100
```

---

## 🎓 总结：三位一体

AlphaZero = **神经网络** + **MCTS** + **自我对弈**

```
       神经网络
      /       \
     /         \
    ↓           ↓
  MCTS  ←→  自我对弈

每个组件都不可或缺：
- 没有网络：MCTS太慢
- 没有MCTS：网络不够精确
- 没有自我对弈：无法持续进步
```

**下一步**：了解实际训练操作 → [04_训练全流程.md](./04_训练全流程.md)
